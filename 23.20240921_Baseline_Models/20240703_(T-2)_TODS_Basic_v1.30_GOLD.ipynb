{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a195f7f-443e-4892-837e-0a4e8c211e09",
   "metadata": {},
   "source": [
    "# <font color = lightcoral>TODS\n",
    ">    \n",
    "### <font color = red> New\n",
    "\n",
    "> Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1d5fb-eee3-4417-9c3e-515c1bc732e3",
   "metadata": {},
   "source": [
    "## <font color = grey> Admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27cdae59-f7da-48a6-b10b-3f16b5d9b48b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import gzip\n",
    "# import time\n",
    "# import warnings\n",
    "# import os\n",
    "# import shutil\n",
    "# from datetime import datetime\n",
    "# # Set display option to show the full length of the column\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Determine the base directory based on the effective user ID\n",
    "# if os.geteuid() == 0:  # root user\n",
    "#     base_dir = '/root'\n",
    "# else:\n",
    "#     base_dir = '/home/sagemaker-user'\n",
    "\n",
    "# # Set the environment variable\n",
    "# os.environ['BASE_DIR'] = base_dir\n",
    "\n",
    "# # Verify the base directory\n",
    "# print(f\"Base directory set to: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d564a94-97fb-4869-bbb6-f5128373be71",
   "metadata": {},
   "source": [
    "# <font color = tomato> Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c4d922-8666-42cc-8812-c559bcec0064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters and Hyperparameters Set\n",
      "****************************************************************************************************\n",
      "\n",
      "Data Preprocessed\n",
      "****************************************************************************************************\n",
      "\n",
      "Created Sequences\n",
      "****************************************************************************************************\n",
      "\n",
      "Split, SMOTE & Train/Test Saved\n",
      "****************************************************************************************************\n",
      "\n",
      "Combined data saved to CSV at /home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS/c20240705_TODS_sample_v1.00.csv\n",
      "Combined data saved to Parquet at /home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS/c20240705_TODS_sample_v1.00.parquet\n"
     ]
    }
   ],
   "source": [
    "# ################################### Parameters and Hyperparameters ################################\n",
    "\n",
    "# # Parameters and Hyperparameters\n",
    "# # data_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/10.20240705_StartOver_NewFeatures/01.Full_Base/'\n",
    "# data_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/10.20240705_StartOver_NewFeatures/03.Sample_Base/'\n",
    "\n",
    "# # input_file = f\"{data_dir}/20240704__full__new_features_v1.10.parquet\"\n",
    "# input_file = f\"{data_dir}/20240704__sample__new_features_v1.50.parquet\"\n",
    "\n",
    "# # output_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/10.20240705_StartOver_NewFeatures/02.Full_Train_Test'tput_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS'\n",
    "# output_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS'\n",
    "\n",
    "# max_events = 20\n",
    "# input_length = 10\n",
    "# gap = 1\n",
    "# prediction_period = 1\n",
    "# test_size = 0.2\n",
    "# shuffle = False\n",
    "\n",
    "# hidden_size = 64\n",
    "# num_layers = 2\n",
    "# output_size = 1\n",
    "# num_epochs = 50\n",
    "# batch_size = 16\n",
    "# learning_rate = 0.001\n",
    "# dropout = 0.5  # Adding dropout to prevent overfitting\n",
    "\n",
    "# print(\"Parameters and Hyperparameters Set\")\n",
    "# print(\"*\"*100 + \"\\n\")\n",
    "\n",
    "# ################################### Data Preprocessing ################################\n",
    "\n",
    "# # Data Preprocessing\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Load the data\n",
    "# df = pd.read_parquet(input_file)\n",
    "\n",
    "# # Select derived features and a subset of EventID columns\n",
    "# selected_columns = ['time_start_int', 'Class', 'unique_events', 'most_frequent_event', 'transitions', 'entropy']\n",
    "# event_id_columns = [col for col in df.columns if col.startswith('EventId_')]\n",
    "\n",
    "# # Replace -1 values with 0\n",
    "# df[event_id_columns].replace(-1, 0, inplace=True)\n",
    "\n",
    "# # Scale numerical features\n",
    "# scaler = StandardScaler()\n",
    "# numerical_features = ['unique_events', 'transitions', 'entropy']\n",
    "# df[selected_columns[2:]] = scaler.fit_transform(df[selected_columns[2:]])\n",
    "\n",
    "# # Encode categorical features\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['most_frequent_event'] = label_encoder.fit_transform(df['most_frequent_event'])\n",
    "\n",
    "# # Apply PCA to EventID columns to reduce them to 50 features\n",
    "# pca = PCA(n_components=max_events)\n",
    "# event_id_pca = pca.fit_transform(df[event_id_columns])\n",
    "\n",
    "# # Create a new DataFrame with the reduced EventID features\n",
    "# event_id_pca_df = pd.DataFrame(event_id_pca, columns=[f'EventId_PCA_{i+1}' for i in range(max_events)])\n",
    "\n",
    "# # Combine the reduced EventID features with the selected columns\n",
    "# df_reduced = pd.concat([df[selected_columns], event_id_pca_df], axis=1)\n",
    "\n",
    "# print(\"Data Preprocessed\")\n",
    "# print(\"*\"*100 + \"\\n\")\n",
    "\n",
    "# ################################### Create Sequences ################################\n",
    "\n",
    "# # Function to create non-overlapping sequences\n",
    "# def create_sequences(data, time_index_col, feature_cols, target_col, input_length=input_length, gap=1, prediction_period=1):\n",
    "#     sequences = []\n",
    "#     targets = []\n",
    "    \n",
    "#     start_idx = 0\n",
    "#     while start_idx + input_length + gap + prediction_period <= len(data):\n",
    "#         end_idx = start_idx + input_length\n",
    "#         sequence = data[feature_cols].iloc[start_idx:end_idx].values\n",
    "#         target = data[target_col].iloc[end_idx + gap:end_idx + gap + prediction_period].values[0]\n",
    "#         sequences.append(sequence)\n",
    "#         targets.append(target)\n",
    "#         # start_idx = end_idx + gap + prediction_period  # Move to the next non-overlapping sequence\n",
    "#         start_idx += 1  # Move to the next row to create overlapping sequences\n",
    "    \n",
    "#     return np.array(sequences), np.array(targets)\n",
    "\n",
    "# # feature_cols = selected_columns + selected_event_ids\n",
    "# feature_cols = [col for col in df_reduced.columns if col != 'Class']\n",
    "# target_col = 'Class'\n",
    "\n",
    "# X, y = create_sequences(df_reduced, 'time_start_int', feature_cols, target_col)\n",
    "\n",
    "# print(\"Created Sequences\")\n",
    "# print(\"*\"*100 + \"\\n\")\n",
    "\n",
    "# ################################### Split & SMOTE ################################\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "# # Flatten X_train to 2D array for SMOTE\n",
    "# X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "# # Apply SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Reshape X_train back to 3D array\n",
    "# X_train = X_train.reshape(-1, input_length, max_events + 5)\n",
    "\n",
    "# # Drop the first column from X_train and X_test\n",
    "# X_train = X_train[:, :, 1:]\n",
    "# X_test = X_test[:, :, 1:]\n",
    "\n",
    "# # # Save the datasets\n",
    "# # np.save(f'{output_dir}/X_train.npy', X_train)\n",
    "# # np.save(f'{output_dir}/X_test.npy', X_test)\n",
    "# # np.save(f'{output_dir}/y_train.npy', y_train)\n",
    "# # np.save(f'{output_dir}/y_test.npy', y_test)\n",
    "\n",
    "# print(\"Split, SMOTE & Train/Test Saved\")\n",
    "# print(\"*\"*100 + \"\\n\")\n",
    "\n",
    "# ################################### Recombine for TODS ################################\n",
    "\n",
    "# # Flatten X_train and X_test to 2D arrays\n",
    "# X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "# X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# # Concatenate X and y for train and test sets\n",
    "# train_data = np.concatenate([X_train_flat, y_train.reshape(-1, 1)], axis=1)\n",
    "# test_data = np.concatenate([X_test_flat, y_test.reshape(-1, 1)], axis=1)\n",
    "\n",
    "# # Combine train and test data\n",
    "# combined_data = np.concatenate([train_data, test_data], axis=0)\n",
    "\n",
    "# # Create a DataFrame\n",
    "# column_names = [f'feature_{i+1}' for i in range(X_train_flat.shape[1])] + ['label']\n",
    "# df_combined = pd.DataFrame(combined_data, columns=column_names)\n",
    "\n",
    "# # Drop feature_2 (time_index)\n",
    "# df_combined = df_combined.drop(columns=['feature_2'])\n",
    "\n",
    "# # Save to CSV\n",
    "# # csv_output_path = f'{output_dir}/c20240705_TODS_full_v1.00.csv'\n",
    "# csv_output_path = f'{output_dir}/c20240705_TODS_sample_v1.00.csv'\n",
    "# df_combined.to_csv(csv_output_path, index=False)\n",
    "\n",
    "# # Save to Parquet\n",
    "# # parquet_output_path = f'{output_dir}/c20240705_TODS_full_v1.00.parquet'\n",
    "# parquet_output_path = f'{output_dir}/c20240705_TODS_sample_v1.00.parquet'\n",
    "\n",
    "# df_combined.to_parquet(parquet_output_path, index=False)\n",
    "\n",
    "# print(f\"Combined data saved to CSV at {csv_output_path}\")\n",
    "# print(f\"Combined data saved to Parquet at {parquet_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab61080-f74f-4225-8fbb-ab5f81ca9df0",
   "metadata": {},
   "source": [
    "### <font color = grey> Review generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a789163e-e294-4475-aa75-1a35560b6709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of the CSV file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>feature_79</th>\n",
       "      <th>feature_80</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>feature_82</th>\n",
       "      <th>feature_83</th>\n",
       "      <th>feature_84</th>\n",
       "      <th>feature_85</th>\n",
       "      <th>feature_86</th>\n",
       "      <th>feature_87</th>\n",
       "      <th>feature_88</th>\n",
       "      <th>feature_89</th>\n",
       "      <th>feature_90</th>\n",
       "      <th>feature_91</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>feature_101</th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "      <th>feature_104</th>\n",
       "      <th>feature_105</th>\n",
       "      <th>feature_106</th>\n",
       "      <th>feature_107</th>\n",
       "      <th>feature_108</th>\n",
       "      <th>feature_109</th>\n",
       "      <th>feature_110</th>\n",
       "      <th>feature_111</th>\n",
       "      <th>feature_112</th>\n",
       "      <th>feature_113</th>\n",
       "      <th>feature_114</th>\n",
       "      <th>feature_115</th>\n",
       "      <th>feature_116</th>\n",
       "      <th>feature_117</th>\n",
       "      <th>feature_118</th>\n",
       "      <th>feature_119</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>feature_130</th>\n",
       "      <th>feature_131</th>\n",
       "      <th>feature_132</th>\n",
       "      <th>feature_133</th>\n",
       "      <th>feature_134</th>\n",
       "      <th>feature_135</th>\n",
       "      <th>feature_136</th>\n",
       "      <th>feature_137</th>\n",
       "      <th>feature_138</th>\n",
       "      <th>feature_139</th>\n",
       "      <th>feature_140</th>\n",
       "      <th>feature_141</th>\n",
       "      <th>feature_142</th>\n",
       "      <th>feature_143</th>\n",
       "      <th>feature_144</th>\n",
       "      <th>feature_145</th>\n",
       "      <th>feature_146</th>\n",
       "      <th>feature_147</th>\n",
       "      <th>feature_148</th>\n",
       "      <th>feature_149</th>\n",
       "      <th>feature_150</th>\n",
       "      <th>feature_151</th>\n",
       "      <th>feature_152</th>\n",
       "      <th>feature_153</th>\n",
       "      <th>feature_154</th>\n",
       "      <th>feature_155</th>\n",
       "      <th>feature_156</th>\n",
       "      <th>feature_157</th>\n",
       "      <th>feature_158</th>\n",
       "      <th>feature_159</th>\n",
       "      <th>feature_160</th>\n",
       "      <th>feature_161</th>\n",
       "      <th>feature_162</th>\n",
       "      <th>feature_163</th>\n",
       "      <th>feature_164</th>\n",
       "      <th>feature_165</th>\n",
       "      <th>feature_166</th>\n",
       "      <th>feature_167</th>\n",
       "      <th>feature_168</th>\n",
       "      <th>feature_169</th>\n",
       "      <th>feature_170</th>\n",
       "      <th>feature_171</th>\n",
       "      <th>feature_172</th>\n",
       "      <th>feature_173</th>\n",
       "      <th>feature_174</th>\n",
       "      <th>feature_175</th>\n",
       "      <th>feature_176</th>\n",
       "      <th>feature_177</th>\n",
       "      <th>feature_178</th>\n",
       "      <th>feature_179</th>\n",
       "      <th>feature_180</th>\n",
       "      <th>feature_181</th>\n",
       "      <th>feature_182</th>\n",
       "      <th>feature_183</th>\n",
       "      <th>feature_184</th>\n",
       "      <th>feature_185</th>\n",
       "      <th>feature_186</th>\n",
       "      <th>feature_187</th>\n",
       "      <th>feature_188</th>\n",
       "      <th>feature_189</th>\n",
       "      <th>feature_190</th>\n",
       "      <th>feature_191</th>\n",
       "      <th>feature_192</th>\n",
       "      <th>feature_193</th>\n",
       "      <th>feature_194</th>\n",
       "      <th>feature_195</th>\n",
       "      <th>feature_196</th>\n",
       "      <th>feature_197</th>\n",
       "      <th>feature_198</th>\n",
       "      <th>feature_199</th>\n",
       "      <th>feature_200</th>\n",
       "      <th>feature_201</th>\n",
       "      <th>feature_202</th>\n",
       "      <th>feature_203</th>\n",
       "      <th>feature_204</th>\n",
       "      <th>feature_205</th>\n",
       "      <th>feature_206</th>\n",
       "      <th>feature_207</th>\n",
       "      <th>feature_208</th>\n",
       "      <th>feature_209</th>\n",
       "      <th>feature_210</th>\n",
       "      <th>feature_211</th>\n",
       "      <th>feature_212</th>\n",
       "      <th>feature_213</th>\n",
       "      <th>feature_214</th>\n",
       "      <th>feature_215</th>\n",
       "      <th>feature_216</th>\n",
       "      <th>feature_217</th>\n",
       "      <th>feature_218</th>\n",
       "      <th>feature_219</th>\n",
       "      <th>feature_220</th>\n",
       "      <th>feature_221</th>\n",
       "      <th>feature_222</th>\n",
       "      <th>feature_223</th>\n",
       "      <th>feature_224</th>\n",
       "      <th>feature_225</th>\n",
       "      <th>feature_226</th>\n",
       "      <th>feature_227</th>\n",
       "      <th>feature_228</th>\n",
       "      <th>feature_229</th>\n",
       "      <th>feature_230</th>\n",
       "      <th>feature_231</th>\n",
       "      <th>feature_232</th>\n",
       "      <th>feature_233</th>\n",
       "      <th>feature_234</th>\n",
       "      <th>feature_235</th>\n",
       "      <th>feature_236</th>\n",
       "      <th>feature_237</th>\n",
       "      <th>feature_238</th>\n",
       "      <th>feature_239</th>\n",
       "      <th>feature_240</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.449756</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-511.899229</td>\n",
       "      <td>-1897.728545</td>\n",
       "      <td>1892.193864</td>\n",
       "      <td>-1800.512951</td>\n",
       "      <td>1716.153147</td>\n",
       "      <td>108.966949</td>\n",
       "      <td>-1087.335503</td>\n",
       "      <td>711.265902</td>\n",
       "      <td>-414.147623</td>\n",
       "      <td>-690.902204</td>\n",
       "      <td>-295.074213</td>\n",
       "      <td>119.688772</td>\n",
       "      <td>-574.414342</td>\n",
       "      <td>18.642667</td>\n",
       "      <td>-334.981288</td>\n",
       "      <td>633.789239</td>\n",
       "      <td>256.889949</td>\n",
       "      <td>235.119026</td>\n",
       "      <td>-61.852877</td>\n",
       "      <td>-85.578348</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.927084</td>\n",
       "      <td>2553.585704</td>\n",
       "      <td>-4906.251012</td>\n",
       "      <td>2335.325503</td>\n",
       "      <td>-314.707397</td>\n",
       "      <td>-1356.950060</td>\n",
       "      <td>-898.198017</td>\n",
       "      <td>-1147.085089</td>\n",
       "      <td>418.328739</td>\n",
       "      <td>371.014384</td>\n",
       "      <td>1076.276228</td>\n",
       "      <td>193.553592</td>\n",
       "      <td>-479.117805</td>\n",
       "      <td>746.671204</td>\n",
       "      <td>-41.674053</td>\n",
       "      <td>1331.949218</td>\n",
       "      <td>-710.600821</td>\n",
       "      <td>155.939896</td>\n",
       "      <td>995.075728</td>\n",
       "      <td>-353.385971</td>\n",
       "      <td>150.517038</td>\n",
       "      <td>-0.063168</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.045745</td>\n",
       "      <td>-0.825372</td>\n",
       "      <td>1792.713630</td>\n",
       "      <td>-4293.303647</td>\n",
       "      <td>2530.837323</td>\n",
       "      <td>-1035.194496</td>\n",
       "      <td>-299.344505</td>\n",
       "      <td>-731.962749</td>\n",
       "      <td>-2155.028632</td>\n",
       "      <td>1078.906461</td>\n",
       "      <td>4.759914</td>\n",
       "      <td>1330.074696</td>\n",
       "      <td>1135.415179</td>\n",
       "      <td>-87.209520</td>\n",
       "      <td>695.512698</td>\n",
       "      <td>-68.084209</td>\n",
       "      <td>537.768010</td>\n",
       "      <td>-433.176891</td>\n",
       "      <td>77.316255</td>\n",
       "      <td>614.537327</td>\n",
       "      <td>-188.355901</td>\n",
       "      <td>152.214179</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>377.955669</td>\n",
       "      <td>-2877.793346</td>\n",
       "      <td>2271.378098</td>\n",
       "      <td>-1653.789303</td>\n",
       "      <td>1059.765109</td>\n",
       "      <td>-264.703657</td>\n",
       "      <td>-2030.153238</td>\n",
       "      <td>1164.539307</td>\n",
       "      <td>-329.725082</td>\n",
       "      <td>541.745058</td>\n",
       "      <td>1003.288816</td>\n",
       "      <td>274.968400</td>\n",
       "      <td>159.332289</td>\n",
       "      <td>-12.997391</td>\n",
       "      <td>-708.439256</td>\n",
       "      <td>457.168845</td>\n",
       "      <td>-4.379625</td>\n",
       "      <td>-362.414328</td>\n",
       "      <td>137.474288</td>\n",
       "      <td>-114.283946</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.256462</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.927084</td>\n",
       "      <td>2553.585704</td>\n",
       "      <td>-4906.251012</td>\n",
       "      <td>2335.325503</td>\n",
       "      <td>-314.707397</td>\n",
       "      <td>-1356.950060</td>\n",
       "      <td>-898.198017</td>\n",
       "      <td>-1147.085089</td>\n",
       "      <td>418.328739</td>\n",
       "      <td>371.014384</td>\n",
       "      <td>1076.276228</td>\n",
       "      <td>193.553592</td>\n",
       "      <td>-479.117805</td>\n",
       "      <td>746.671204</td>\n",
       "      <td>-41.674053</td>\n",
       "      <td>1331.949218</td>\n",
       "      <td>-710.600821</td>\n",
       "      <td>155.939896</td>\n",
       "      <td>995.075728</td>\n",
       "      <td>-353.385971</td>\n",
       "      <td>150.517038</td>\n",
       "      <td>-0.063168</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.045745</td>\n",
       "      <td>-0.825372</td>\n",
       "      <td>1792.713630</td>\n",
       "      <td>-4293.303647</td>\n",
       "      <td>2530.837323</td>\n",
       "      <td>-1035.194496</td>\n",
       "      <td>-299.344505</td>\n",
       "      <td>-731.962749</td>\n",
       "      <td>-2155.028632</td>\n",
       "      <td>1078.906461</td>\n",
       "      <td>4.759914</td>\n",
       "      <td>1330.074696</td>\n",
       "      <td>1135.415179</td>\n",
       "      <td>-87.209520</td>\n",
       "      <td>695.512698</td>\n",
       "      <td>-68.084209</td>\n",
       "      <td>537.768010</td>\n",
       "      <td>-433.176891</td>\n",
       "      <td>77.316255</td>\n",
       "      <td>614.537327</td>\n",
       "      <td>-188.355901</td>\n",
       "      <td>152.214179</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>377.955669</td>\n",
       "      <td>-2877.793346</td>\n",
       "      <td>2271.378098</td>\n",
       "      <td>-1653.789303</td>\n",
       "      <td>1059.765109</td>\n",
       "      <td>-264.703657</td>\n",
       "      <td>-2030.153238</td>\n",
       "      <td>1164.539307</td>\n",
       "      <td>-329.725082</td>\n",
       "      <td>541.745058</td>\n",
       "      <td>1003.288816</td>\n",
       "      <td>274.968400</td>\n",
       "      <td>159.332289</td>\n",
       "      <td>-12.997391</td>\n",
       "      <td>-708.439256</td>\n",
       "      <td>457.168845</td>\n",
       "      <td>-4.379625</td>\n",
       "      <td>-362.414328</td>\n",
       "      <td>137.474288</td>\n",
       "      <td>-114.283946</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2653.992815</td>\n",
       "      <td>939.153282</td>\n",
       "      <td>-509.806102</td>\n",
       "      <td>441.657659</td>\n",
       "      <td>-494.669864</td>\n",
       "      <td>-98.464612</td>\n",
       "      <td>-332.834199</td>\n",
       "      <td>180.498042</td>\n",
       "      <td>-83.460820</td>\n",
       "      <td>-182.993894</td>\n",
       "      <td>-152.785112</td>\n",
       "      <td>-41.609244</td>\n",
       "      <td>241.328905</td>\n",
       "      <td>-9.520720</td>\n",
       "      <td>-63.348823</td>\n",
       "      <td>118.068359</td>\n",
       "      <td>72.076774</td>\n",
       "      <td>114.546084</td>\n",
       "      <td>-50.421345</td>\n",
       "      <td>14.769983</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.063168</td>\n",
       "      <td>-0.045745</td>\n",
       "      <td>-0.825372</td>\n",
       "      <td>1792.713630</td>\n",
       "      <td>-4293.303647</td>\n",
       "      <td>2530.837323</td>\n",
       "      <td>-1035.194496</td>\n",
       "      <td>-299.344505</td>\n",
       "      <td>-731.962749</td>\n",
       "      <td>-2155.028632</td>\n",
       "      <td>1078.906461</td>\n",
       "      <td>4.759914</td>\n",
       "      <td>1330.074696</td>\n",
       "      <td>1135.415179</td>\n",
       "      <td>-87.209520</td>\n",
       "      <td>695.512698</td>\n",
       "      <td>-68.084209</td>\n",
       "      <td>537.768010</td>\n",
       "      <td>-433.176891</td>\n",
       "      <td>77.316255</td>\n",
       "      <td>614.537327</td>\n",
       "      <td>-188.355901</td>\n",
       "      <td>152.214179</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>377.955669</td>\n",
       "      <td>-2877.793346</td>\n",
       "      <td>2271.378098</td>\n",
       "      <td>-1653.789303</td>\n",
       "      <td>1059.765109</td>\n",
       "      <td>-264.703657</td>\n",
       "      <td>-2030.153238</td>\n",
       "      <td>1164.539307</td>\n",
       "      <td>-329.725082</td>\n",
       "      <td>541.745058</td>\n",
       "      <td>1003.288816</td>\n",
       "      <td>274.968400</td>\n",
       "      <td>159.332289</td>\n",
       "      <td>-12.997391</td>\n",
       "      <td>-708.439256</td>\n",
       "      <td>457.168845</td>\n",
       "      <td>-4.379625</td>\n",
       "      <td>-362.414328</td>\n",
       "      <td>137.474288</td>\n",
       "      <td>-114.283946</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2653.992815</td>\n",
       "      <td>939.153282</td>\n",
       "      <td>-509.806102</td>\n",
       "      <td>441.657659</td>\n",
       "      <td>-494.669864</td>\n",
       "      <td>-98.464612</td>\n",
       "      <td>-332.834199</td>\n",
       "      <td>180.498042</td>\n",
       "      <td>-83.460820</td>\n",
       "      <td>-182.993894</td>\n",
       "      <td>-152.785112</td>\n",
       "      <td>-41.609244</td>\n",
       "      <td>241.328905</td>\n",
       "      <td>-9.520720</td>\n",
       "      <td>-63.348823</td>\n",
       "      <td>118.068359</td>\n",
       "      <td>72.076774</td>\n",
       "      <td>114.546084</td>\n",
       "      <td>-50.421345</td>\n",
       "      <td>14.769983</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.040219</td>\n",
       "      <td>0.367475</td>\n",
       "      <td>-1654.273947</td>\n",
       "      <td>-495.122135</td>\n",
       "      <td>1021.132498</td>\n",
       "      <td>-1450.933450</td>\n",
       "      <td>1928.957501</td>\n",
       "      <td>625.990570</td>\n",
       "      <td>2400.454654</td>\n",
       "      <td>-1457.796315</td>\n",
       "      <td>619.236134</td>\n",
       "      <td>1112.530577</td>\n",
       "      <td>807.054343</td>\n",
       "      <td>95.196853</td>\n",
       "      <td>-122.652117</td>\n",
       "      <td>-17.198895</td>\n",
       "      <td>-71.727181</td>\n",
       "      <td>-1.866234</td>\n",
       "      <td>-81.757981</td>\n",
       "      <td>-239.064685</td>\n",
       "      <td>130.980977</td>\n",
       "      <td>-94.438973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.449756</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>377.955669</td>\n",
       "      <td>-2877.793346</td>\n",
       "      <td>2271.378098</td>\n",
       "      <td>-1653.789303</td>\n",
       "      <td>1059.765109</td>\n",
       "      <td>-264.703657</td>\n",
       "      <td>-2030.153238</td>\n",
       "      <td>1164.539307</td>\n",
       "      <td>-329.725082</td>\n",
       "      <td>541.745058</td>\n",
       "      <td>1003.288816</td>\n",
       "      <td>274.968400</td>\n",
       "      <td>159.332289</td>\n",
       "      <td>-12.997391</td>\n",
       "      <td>-708.439256</td>\n",
       "      <td>457.168845</td>\n",
       "      <td>-4.379625</td>\n",
       "      <td>-362.414328</td>\n",
       "      <td>137.474288</td>\n",
       "      <td>-114.283946</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2653.992815</td>\n",
       "      <td>939.153282</td>\n",
       "      <td>-509.806102</td>\n",
       "      <td>441.657659</td>\n",
       "      <td>-494.669864</td>\n",
       "      <td>-98.464612</td>\n",
       "      <td>-332.834199</td>\n",
       "      <td>180.498042</td>\n",
       "      <td>-83.460820</td>\n",
       "      <td>-182.993894</td>\n",
       "      <td>-152.785112</td>\n",
       "      <td>-41.609244</td>\n",
       "      <td>241.328905</td>\n",
       "      <td>-9.520720</td>\n",
       "      <td>-63.348823</td>\n",
       "      <td>118.068359</td>\n",
       "      <td>72.076774</td>\n",
       "      <td>114.546084</td>\n",
       "      <td>-50.421345</td>\n",
       "      <td>14.769983</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.040219</td>\n",
       "      <td>0.367475</td>\n",
       "      <td>-1654.273947</td>\n",
       "      <td>-495.122135</td>\n",
       "      <td>1021.132498</td>\n",
       "      <td>-1450.933450</td>\n",
       "      <td>1928.957501</td>\n",
       "      <td>625.990570</td>\n",
       "      <td>2400.454654</td>\n",
       "      <td>-1457.796315</td>\n",
       "      <td>619.236134</td>\n",
       "      <td>1112.530577</td>\n",
       "      <td>807.054343</td>\n",
       "      <td>95.196853</td>\n",
       "      <td>-122.652117</td>\n",
       "      <td>-17.198895</td>\n",
       "      <td>-71.727181</td>\n",
       "      <td>-1.866234</td>\n",
       "      <td>-81.757981</td>\n",
       "      <td>-239.064685</td>\n",
       "      <td>130.980977</td>\n",
       "      <td>-94.438973</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>774.321552</td>\n",
       "      <td>-3620.477184</td>\n",
       "      <td>3399.587477</td>\n",
       "      <td>-3265.738566</td>\n",
       "      <td>3222.444508</td>\n",
       "      <td>291.253257</td>\n",
       "      <td>-1411.265434</td>\n",
       "      <td>968.796562</td>\n",
       "      <td>-623.566475</td>\n",
       "      <td>-1164.124057</td>\n",
       "      <td>-574.162339</td>\n",
       "      <td>184.682239</td>\n",
       "      <td>-1144.981803</td>\n",
       "      <td>49.973985</td>\n",
       "      <td>-369.112570</td>\n",
       "      <td>862.690530</td>\n",
       "      <td>374.757827</td>\n",
       "      <td>389.828247</td>\n",
       "      <td>-118.692069</td>\n",
       "      <td>-112.547679</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.516713</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2653.992815</td>\n",
       "      <td>939.153282</td>\n",
       "      <td>-509.806102</td>\n",
       "      <td>441.657659</td>\n",
       "      <td>-494.669864</td>\n",
       "      <td>-98.464612</td>\n",
       "      <td>-332.834199</td>\n",
       "      <td>180.498042</td>\n",
       "      <td>-83.460820</td>\n",
       "      <td>-182.993894</td>\n",
       "      <td>-152.785112</td>\n",
       "      <td>-41.609244</td>\n",
       "      <td>241.328905</td>\n",
       "      <td>-9.520720</td>\n",
       "      <td>-63.348823</td>\n",
       "      <td>118.068359</td>\n",
       "      <td>72.076774</td>\n",
       "      <td>114.546084</td>\n",
       "      <td>-50.421345</td>\n",
       "      <td>14.769983</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.040219</td>\n",
       "      <td>0.367475</td>\n",
       "      <td>-1654.273947</td>\n",
       "      <td>-495.122135</td>\n",
       "      <td>1021.132498</td>\n",
       "      <td>-1450.933450</td>\n",
       "      <td>1928.957501</td>\n",
       "      <td>625.990570</td>\n",
       "      <td>2400.454654</td>\n",
       "      <td>-1457.796315</td>\n",
       "      <td>619.236134</td>\n",
       "      <td>1112.530577</td>\n",
       "      <td>807.054343</td>\n",
       "      <td>95.196853</td>\n",
       "      <td>-122.652117</td>\n",
       "      <td>-17.198895</td>\n",
       "      <td>-71.727181</td>\n",
       "      <td>-1.866234</td>\n",
       "      <td>-81.757981</td>\n",
       "      <td>-239.064685</td>\n",
       "      <td>130.980977</td>\n",
       "      <td>-94.438973</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>774.321552</td>\n",
       "      <td>-3620.477184</td>\n",
       "      <td>3399.587477</td>\n",
       "      <td>-3265.738566</td>\n",
       "      <td>3222.444508</td>\n",
       "      <td>291.253257</td>\n",
       "      <td>-1411.265434</td>\n",
       "      <td>968.796562</td>\n",
       "      <td>-623.566475</td>\n",
       "      <td>-1164.124057</td>\n",
       "      <td>-574.162339</td>\n",
       "      <td>184.682239</td>\n",
       "      <td>-1144.981803</td>\n",
       "      <td>49.973985</td>\n",
       "      <td>-369.112570</td>\n",
       "      <td>862.690530</td>\n",
       "      <td>374.757827</td>\n",
       "      <td>389.828247</td>\n",
       "      <td>-118.692069</td>\n",
       "      <td>-112.547679</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>16629.491533</td>\n",
       "      <td>-9161.244843</td>\n",
       "      <td>-6259.082448</td>\n",
       "      <td>4333.526148</td>\n",
       "      <td>2619.189006</td>\n",
       "      <td>3768.893527</td>\n",
       "      <td>-412.784469</td>\n",
       "      <td>1104.611833</td>\n",
       "      <td>-991.265603</td>\n",
       "      <td>1415.507050</td>\n",
       "      <td>-923.452227</td>\n",
       "      <td>-2066.367857</td>\n",
       "      <td>-178.521065</td>\n",
       "      <td>69.490596</td>\n",
       "      <td>-841.153826</td>\n",
       "      <td>-75.882355</td>\n",
       "      <td>-240.844249</td>\n",
       "      <td>51.920256</td>\n",
       "      <td>-273.459169</td>\n",
       "      <td>524.104264</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_3  feature_4    feature_5    feature_6    feature_7  \\\n",
       "0  -0.449756  -0.071532  -0.935017  -511.899229 -1897.728545  1892.193864   \n",
       "1  -0.256462  -0.067848  -0.927084  2553.585704 -4906.251012  2335.325503   \n",
       "2  -0.063168  -0.045745  -0.825372  1792.713630 -4293.303647  2530.837323   \n",
       "3  -0.449756  -0.071532  -0.935017   377.955669 -2877.793346  2271.378098   \n",
       "4   0.516713  -0.060481  -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "\n",
       "     feature_8    feature_9  feature_10   feature_11   feature_12  feature_13  \\\n",
       "0 -1800.512951  1716.153147  108.966949 -1087.335503   711.265902 -414.147623   \n",
       "1  -314.707397 -1356.950060 -898.198017 -1147.085089   418.328739  371.014384   \n",
       "2 -1035.194496  -299.344505 -731.962749 -2155.028632  1078.906461    4.759914   \n",
       "3 -1653.789303  1059.765109 -264.703657 -2030.153238  1164.539307 -329.725082   \n",
       "4 -1730.362375  1888.873593  287.655745  -303.580668   291.213129 -339.514773   \n",
       "\n",
       "    feature_14   feature_15  feature_16  feature_17  feature_18   feature_19  \\\n",
       "0  -690.902204  -295.074213  119.688772 -574.414342   18.642667  -334.981288   \n",
       "1  1076.276228   193.553592 -479.117805  746.671204  -41.674053  1331.949218   \n",
       "2  1330.074696  1135.415179  -87.209520  695.512698  -68.084209   537.768010   \n",
       "3   541.745058  1003.288816  274.968400  159.332289  -12.997391  -708.439256   \n",
       "4 -1138.600817  -963.117321  -72.764146 -446.154337   32.807479   274.144662   \n",
       "\n",
       "   feature_20  feature_21  feature_22  feature_23  feature_24  feature_25  \\\n",
       "0  633.789239  256.889949  235.119026  -61.852877  -85.578348   -0.256462   \n",
       "1 -710.600821  155.939896  995.075728 -353.385971  150.517038   -0.063168   \n",
       "2 -433.176891   77.316255  614.537327 -188.355901  152.214179   -0.449756   \n",
       "3  457.168845   -4.379625 -362.414328  137.474288 -114.283946    0.516713   \n",
       "4  -14.479703  113.404196  360.894652 -152.953085   95.524119   -0.256462   \n",
       "\n",
       "   feature_26  feature_27  feature_28   feature_29   feature_30   feature_31  \\\n",
       "0        36.0   -0.067848   -0.927084  2553.585704 -4906.251012  2335.325503   \n",
       "1        36.0   -0.045745   -0.825372  1792.713630 -4293.303647  2530.837323   \n",
       "2        36.0   -0.071532   -0.935017   377.955669 -2877.793346  2271.378098   \n",
       "3        36.0   -0.060481   -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "4       133.0   -0.067848   -0.037538 -2586.122444   848.118498  -420.123075   \n",
       "\n",
       "    feature_32   feature_33  feature_34   feature_35   feature_36  feature_37  \\\n",
       "0  -314.707397 -1356.950060 -898.198017 -1147.085089   418.328739  371.014384   \n",
       "1 -1035.194496  -299.344505 -731.962749 -2155.028632  1078.906461    4.759914   \n",
       "2 -1653.789303  1059.765109 -264.703657 -2030.153238  1164.539307 -329.725082   \n",
       "3 -1730.362375  1888.873593  287.655745  -303.580668   291.213129 -339.514773   \n",
       "4   326.795783  -347.256097  -44.591849   -80.509364     9.551384    6.428320   \n",
       "\n",
       "    feature_38   feature_39  feature_40  feature_41  feature_42   feature_43  \\\n",
       "0  1076.276228   193.553592 -479.117805  746.671204  -41.674053  1331.949218   \n",
       "1  1330.074696  1135.415179  -87.209520  695.512698  -68.084209   537.768010   \n",
       "2   541.745058  1003.288816  274.968400  159.332289  -12.997391  -708.439256   \n",
       "3 -1138.600817  -963.117321  -72.764146 -446.154337   32.807479   274.144662   \n",
       "4    46.183642    39.418842    2.467569 -378.475560   -7.389064   128.475564   \n",
       "\n",
       "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \\\n",
       "0 -710.600821  155.939896  995.075728 -353.385971  150.517038   -0.063168   \n",
       "1 -433.176891   77.316255  614.537327 -188.355901  152.214179   -0.449756   \n",
       "2  457.168845   -4.379625 -362.414328  137.474288 -114.283946    0.516713   \n",
       "3  -14.479703  113.404196  360.894652 -152.953085   95.524119   -0.256462   \n",
       "4 -311.480012 -199.212245 -382.762411  160.599969  -95.361173    0.516713   \n",
       "\n",
       "   feature_50  feature_51  feature_52   feature_53   feature_54   feature_55  \\\n",
       "0        36.0   -0.045745   -0.825372  1792.713630 -4293.303647  2530.837323   \n",
       "1        36.0   -0.071532   -0.935017   377.955669 -2877.793346  2271.378098   \n",
       "2        36.0   -0.060481   -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "3       133.0   -0.067848   -0.037538 -2586.122444   848.118498  -420.123075   \n",
       "4        60.0   -0.038377    0.047929 -2254.853445   363.563256   117.791524   \n",
       "\n",
       "    feature_56   feature_57  feature_58   feature_59   feature_60  feature_61  \\\n",
       "0 -1035.194496  -299.344505 -731.962749 -2155.028632  1078.906461    4.759914   \n",
       "1 -1653.789303  1059.765109 -264.703657 -2030.153238  1164.539307 -329.725082   \n",
       "2 -1730.362375  1888.873593  287.655745  -303.580668   291.213129 -339.514773   \n",
       "3   326.795783  -347.256097  -44.591849   -80.509364     9.551384    6.428320   \n",
       "4  -356.372051   554.798155  246.224350  1126.833343  -717.049501  345.098899   \n",
       "\n",
       "    feature_62   feature_63  feature_64  feature_65  feature_66  feature_67  \\\n",
       "0  1330.074696  1135.415179  -87.209520  695.512698  -68.084209  537.768010   \n",
       "1   541.745058  1003.288816  274.968400  159.332289  -12.997391 -708.439256   \n",
       "2 -1138.600817  -963.117321  -72.764146 -446.154337   32.807479  274.144662   \n",
       "3    46.183642    39.418842    2.467569 -378.475560   -7.389064  128.475564   \n",
       "4   764.424331   648.789253  139.082896 -595.637608   11.603653  -51.091916   \n",
       "\n",
       "   feature_68  feature_69  feature_70  feature_71  feature_72  feature_73  \\\n",
       "0 -433.176891   77.316255  614.537327 -188.355901  152.214179   -0.449756   \n",
       "1  457.168845   -4.379625 -362.414328  137.474288 -114.283946    0.516713   \n",
       "2  -14.479703  113.404196  360.894652 -152.953085   95.524119   -0.256462   \n",
       "3 -311.480012 -199.212245 -382.762411  160.599969  -95.361173    0.516713   \n",
       "4  329.033295  265.275586  539.978242 -224.529041   88.602621    0.323419   \n",
       "\n",
       "   feature_74  feature_75  feature_76   feature_77   feature_78   feature_79  \\\n",
       "0        36.0   -0.071532   -0.935017   377.955669 -2877.793346  2271.378098   \n",
       "1        36.0   -0.060481   -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "2       133.0   -0.067848   -0.037538 -2586.122444   848.118498  -420.123075   \n",
       "3        60.0   -0.038377    0.047929 -2254.853445   363.563256   117.791524   \n",
       "4        50.0   -0.060481    0.624603 -2333.513215   482.056503   -16.367345   \n",
       "\n",
       "    feature_80   feature_81  feature_82   feature_83   feature_84  feature_85  \\\n",
       "0 -1653.789303  1059.765109 -264.703657 -2030.153238  1164.539307 -329.725082   \n",
       "1 -1730.362375  1888.873593  287.655745  -303.580668   291.213129 -339.514773   \n",
       "2   326.795783  -347.256097  -44.591849   -80.509364     9.551384    6.428320   \n",
       "3  -356.372051   554.798155  246.224350  1126.833343  -717.049501  345.098899   \n",
       "4  -190.581365   336.703963  183.078889   901.774797  -595.835087  297.040540   \n",
       "\n",
       "    feature_86   feature_87  feature_88   feature_89  feature_90  feature_91  \\\n",
       "0   541.745058  1003.288816  274.968400   159.332289  -12.997391 -708.439256   \n",
       "1 -1138.600817  -963.117321  -72.764146  -446.154337   32.807479  274.144662   \n",
       "2    46.183642    39.418842    2.467569  -378.475560   -7.389064  128.475564   \n",
       "3   764.424331   648.789253  139.082896  -595.637608   11.603653  -51.091916   \n",
       "4   709.857510   620.251674  150.557987 -1068.140880   15.439787  161.697588   \n",
       "\n",
       "   feature_92  feature_93  feature_94  feature_95  feature_96  feature_97  \\\n",
       "0  457.168845   -4.379625 -362.414328  137.474288 -114.283946    0.516713   \n",
       "1  -14.479703  113.404196  360.894652 -152.953085   95.524119   -0.256462   \n",
       "2 -311.480012 -199.212245 -382.762411  160.599969  -95.361173    0.516713   \n",
       "3  329.033295  265.275586  539.978242 -224.529041   88.602621    0.323419   \n",
       "4 -196.871609  -69.693087  -61.658019   22.072367   -5.119332    0.130125   \n",
       "\n",
       "   feature_98  feature_99  feature_100  feature_101  feature_102  feature_103  \\\n",
       "0        36.0   -0.060481    -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "1       133.0   -0.067848    -0.037538 -2586.122444   848.118498  -420.123075   \n",
       "2        60.0   -0.038377     0.047929 -2254.853445   363.563256   117.791524   \n",
       "3        50.0   -0.060481     0.624603 -2333.513215   482.056503   -16.367345   \n",
       "4        60.0   -0.066006     0.408094 -2594.886918   856.392628  -422.935077   \n",
       "\n",
       "   feature_104  feature_105  feature_106  feature_107  feature_108  \\\n",
       "0 -1730.362375  1888.873593   287.655745  -303.580668   291.213129   \n",
       "1   326.795783  -347.256097   -44.591849   -80.509364     9.551384   \n",
       "2  -356.372051   554.798155   246.224350  1126.833343  -717.049501   \n",
       "3  -190.581365   336.703963   183.078889   901.774797  -595.835087   \n",
       "4   327.179150  -343.289108   -43.757904   -76.624347    11.753169   \n",
       "\n",
       "   feature_109  feature_110  feature_111  feature_112  feature_113  \\\n",
       "0  -339.514773 -1138.600817  -963.117321   -72.764146  -446.154337   \n",
       "1     6.428320    46.183642    39.418842     2.467569  -378.475560   \n",
       "2   345.098899   764.424331   648.789253   139.082896  -595.637608   \n",
       "3   297.040540   709.857510   620.251674   150.557987 -1068.140880   \n",
       "4     5.125490    49.332523    52.623456    15.897757  -337.693623   \n",
       "\n",
       "   feature_114  feature_115  feature_116  feature_117  feature_118  \\\n",
       "0    32.807479   274.144662   -14.479703   113.404196   360.894652   \n",
       "1    -7.389064   128.475564  -311.480012  -199.212245  -382.762411   \n",
       "2    11.603653   -51.091916   329.033295   265.275586   539.978242   \n",
       "3    15.439787   161.697588  -196.871609   -69.693087   -61.658019   \n",
       "4    -2.546881   116.051452  -264.638371  -167.607511  -297.073758   \n",
       "\n",
       "   feature_119  feature_120  feature_121  feature_122  feature_123  \\\n",
       "0  -152.953085    95.524119    -0.256462        133.0    -0.067848   \n",
       "1   160.599969   -95.361173     0.516713         60.0    -0.038377   \n",
       "2  -224.529041    88.602621     0.323419         50.0    -0.060481   \n",
       "3    22.072367    -5.119332     0.130125         60.0    -0.066006   \n",
       "4   125.329250   -53.256074    -0.449756        102.0    -0.071532   \n",
       "\n",
       "   feature_124  feature_125  feature_126  feature_127  feature_128  \\\n",
       "0    -0.037538 -2586.122444   848.118498  -420.123075   326.795783   \n",
       "1     0.047929 -2254.853445   363.563256   117.791524  -356.372051   \n",
       "2     0.624603 -2333.513215   482.056503   -16.367345  -190.581365   \n",
       "3     0.408094 -2594.886918   856.392628  -422.935077   327.179150   \n",
       "4    -0.935017 -2651.099060   935.838092  -507.312366   439.012905   \n",
       "\n",
       "   feature_129  feature_130  feature_131  feature_132  feature_133  \\\n",
       "0  -347.256097   -44.591849   -80.509364     9.551384     6.428320   \n",
       "1   554.798155   246.224350  1126.833343  -717.049501   345.098899   \n",
       "2   336.703963   183.078889   901.774797  -595.835087   297.040540   \n",
       "3  -343.289108   -43.757904   -76.624347    11.753169     5.125490   \n",
       "4  -492.002428   -97.434539  -328.253610   177.183495   -82.005992   \n",
       "\n",
       "   feature_134  feature_135  feature_136  feature_137  feature_138  \\\n",
       "0    46.183642    39.418842     2.467569  -378.475560    -7.389064   \n",
       "1   764.424331   648.789253   139.082896  -595.637608    11.603653   \n",
       "2   709.857510   620.251674   150.557987 -1068.140880    15.439787   \n",
       "3    49.332523    52.623456    15.897757  -337.693623    -2.546881   \n",
       "4  -182.494875  -154.698858   -44.773472   237.266880   -11.527485   \n",
       "\n",
       "   feature_139  feature_140  feature_141  feature_142  feature_143  \\\n",
       "0   128.475564  -311.480012  -199.212245  -382.762411   160.599969   \n",
       "1   -51.091916   329.033295   265.275586   539.978242  -224.529041   \n",
       "2   161.697588  -196.871609   -69.693087   -61.658019    22.072367   \n",
       "3   116.051452  -264.638371  -167.607511  -297.073758   125.329250   \n",
       "4   -63.793977   118.925845    74.088607   113.187022   -50.485476   \n",
       "\n",
       "   feature_144  feature_145  feature_146  feature_147  feature_148  \\\n",
       "0   -95.361173     0.516713         60.0    -0.038377     0.047929   \n",
       "1    88.602621     0.323419         50.0    -0.060481     0.624603   \n",
       "2    -5.119332     0.130125         60.0    -0.066006     0.408094   \n",
       "3   -53.256074    -0.449756        102.0    -0.071532    -0.935017   \n",
       "4     9.821345    -0.449756         56.0    -0.071532    -0.935017   \n",
       "\n",
       "   feature_149  feature_150  feature_151  feature_152  feature_153  \\\n",
       "0 -2254.853445   363.563256   117.791524  -356.372051   554.798155   \n",
       "1 -2333.513215   482.056503   -16.367345  -190.581365   336.703963   \n",
       "2 -2594.886918   856.392628  -422.935077   327.179150  -343.289108   \n",
       "3 -2651.099060   935.838092  -507.312366   439.012905  -492.002428   \n",
       "4 -2653.992815   939.153282  -509.806102   441.657659  -494.669864   \n",
       "\n",
       "   feature_154  feature_155  feature_156  feature_157  feature_158  \\\n",
       "0   246.224350  1126.833343  -717.049501   345.098899   764.424331   \n",
       "1   183.078889   901.774797  -595.835087   297.040540   709.857510   \n",
       "2   -43.757904   -76.624347    11.753169     5.125490    49.332523   \n",
       "3   -97.434539  -328.253610   177.183495   -82.005992  -182.494875   \n",
       "4   -98.464612  -332.834199   180.498042   -83.460820  -182.993894   \n",
       "\n",
       "   feature_159  feature_160  feature_161  feature_162  feature_163  \\\n",
       "0   648.789253   139.082896  -595.637608    11.603653   -51.091916   \n",
       "1   620.251674   150.557987 -1068.140880    15.439787   161.697588   \n",
       "2    52.623456    15.897757  -337.693623    -2.546881   116.051452   \n",
       "3  -154.698858   -44.773472   237.266880   -11.527485   -63.793977   \n",
       "4  -152.785112   -41.609244   241.328905    -9.520720   -63.348823   \n",
       "\n",
       "   feature_164  feature_165  feature_166  feature_167  feature_168  \\\n",
       "0   329.033295   265.275586   539.978242  -224.529041    88.602621   \n",
       "1  -196.871609   -69.693087   -61.658019    22.072367    -5.119332   \n",
       "2  -264.638371  -167.607511  -297.073758   125.329250   -53.256074   \n",
       "3   118.925845    74.088607   113.187022   -50.485476     9.821345   \n",
       "4   118.068359    72.076774   114.546084   -50.421345    14.769983   \n",
       "\n",
       "   feature_169  feature_170  feature_171  feature_172  feature_173  \\\n",
       "0     0.323419         50.0    -0.060481     0.624603 -2333.513215   \n",
       "1     0.130125         60.0    -0.066006     0.408094 -2594.886918   \n",
       "2    -0.449756        102.0    -0.071532    -0.935017 -2651.099060   \n",
       "3    -0.449756         56.0    -0.071532    -0.935017 -2653.992815   \n",
       "4     0.516713         56.0    -0.040219     0.367475 -1654.273947   \n",
       "\n",
       "   feature_174  feature_175  feature_176  feature_177  feature_178  \\\n",
       "0   482.056503   -16.367345  -190.581365   336.703963   183.078889   \n",
       "1   856.392628  -422.935077   327.179150  -343.289108   -43.757904   \n",
       "2   935.838092  -507.312366   439.012905  -492.002428   -97.434539   \n",
       "3   939.153282  -509.806102   441.657659  -494.669864   -98.464612   \n",
       "4  -495.122135  1021.132498 -1450.933450  1928.957501   625.990570   \n",
       "\n",
       "   feature_179  feature_180  feature_181  feature_182  feature_183  \\\n",
       "0   901.774797  -595.835087   297.040540   709.857510   620.251674   \n",
       "1   -76.624347    11.753169     5.125490    49.332523    52.623456   \n",
       "2  -328.253610   177.183495   -82.005992  -182.494875  -154.698858   \n",
       "3  -332.834199   180.498042   -83.460820  -182.993894  -152.785112   \n",
       "4  2400.454654 -1457.796315   619.236134  1112.530577   807.054343   \n",
       "\n",
       "   feature_184  feature_185  feature_186  feature_187  feature_188  \\\n",
       "0   150.557987 -1068.140880    15.439787   161.697588  -196.871609   \n",
       "1    15.897757  -337.693623    -2.546881   116.051452  -264.638371   \n",
       "2   -44.773472   237.266880   -11.527485   -63.793977   118.925845   \n",
       "3   -41.609244   241.328905    -9.520720   -63.348823   118.068359   \n",
       "4    95.196853  -122.652117   -17.198895   -71.727181    -1.866234   \n",
       "\n",
       "   feature_189  feature_190  feature_191  feature_192  feature_193  \\\n",
       "0   -69.693087   -61.658019    22.072367    -5.119332     0.130125   \n",
       "1  -167.607511  -297.073758   125.329250   -53.256074    -0.449756   \n",
       "2    74.088607   113.187022   -50.485476     9.821345    -0.449756   \n",
       "3    72.076774   114.546084   -50.421345    14.769983     0.516713   \n",
       "4   -81.757981  -239.064685   130.980977   -94.438973    -0.449756   \n",
       "\n",
       "   feature_194  feature_195  feature_196  feature_197  feature_198  \\\n",
       "0         60.0    -0.066006     0.408094 -2594.886918   856.392628   \n",
       "1        102.0    -0.071532    -0.935017 -2651.099060   935.838092   \n",
       "2         56.0    -0.071532    -0.935017 -2653.992815   939.153282   \n",
       "3         56.0    -0.040219     0.367475 -1654.273947  -495.122135   \n",
       "4         56.0    -0.071532    -0.935017   774.321552 -3620.477184   \n",
       "\n",
       "   feature_199  feature_200  feature_201  feature_202  feature_203  \\\n",
       "0  -422.935077   327.179150  -343.289108   -43.757904   -76.624347   \n",
       "1  -507.312366   439.012905  -492.002428   -97.434539  -328.253610   \n",
       "2  -509.806102   441.657659  -494.669864   -98.464612  -332.834199   \n",
       "3  1021.132498 -1450.933450  1928.957501   625.990570  2400.454654   \n",
       "4  3399.587477 -3265.738566  3222.444508   291.253257 -1411.265434   \n",
       "\n",
       "   feature_204  feature_205  feature_206  feature_207  feature_208  \\\n",
       "0    11.753169     5.125490    49.332523    52.623456    15.897757   \n",
       "1   177.183495   -82.005992  -182.494875  -154.698858   -44.773472   \n",
       "2   180.498042   -83.460820  -182.993894  -152.785112   -41.609244   \n",
       "3 -1457.796315   619.236134  1112.530577   807.054343    95.196853   \n",
       "4   968.796562  -623.566475 -1164.124057  -574.162339   184.682239   \n",
       "\n",
       "   feature_209  feature_210  feature_211  feature_212  feature_213  \\\n",
       "0  -337.693623    -2.546881   116.051452  -264.638371  -167.607511   \n",
       "1   237.266880   -11.527485   -63.793977   118.925845    74.088607   \n",
       "2   241.328905    -9.520720   -63.348823   118.068359    72.076774   \n",
       "3  -122.652117   -17.198895   -71.727181    -1.866234   -81.757981   \n",
       "4 -1144.981803    49.973985  -369.112570   862.690530   374.757827   \n",
       "\n",
       "   feature_214  feature_215  feature_216  feature_217  feature_218  \\\n",
       "0  -297.073758   125.329250   -53.256074    -0.449756        102.0   \n",
       "1   113.187022   -50.485476     9.821345    -0.449756         56.0   \n",
       "2   114.546084   -50.421345    14.769983     0.516713         56.0   \n",
       "3  -239.064685   130.980977   -94.438973    -0.449756         56.0   \n",
       "4   389.828247  -118.692069  -112.547679    -0.449756         56.0   \n",
       "\n",
       "   feature_219  feature_220   feature_221  feature_222  feature_223  \\\n",
       "0    -0.071532    -0.935017  -2651.099060   935.838092  -507.312366   \n",
       "1    -0.071532    -0.935017  -2653.992815   939.153282  -509.806102   \n",
       "2    -0.040219     0.367475  -1654.273947  -495.122135  1021.132498   \n",
       "3    -0.071532    -0.935017    774.321552 -3620.477184  3399.587477   \n",
       "4    -0.071532    -0.935017  16629.491533 -9161.244843 -6259.082448   \n",
       "\n",
       "   feature_224  feature_225  feature_226  feature_227  feature_228  \\\n",
       "0   439.012905  -492.002428   -97.434539  -328.253610   177.183495   \n",
       "1   441.657659  -494.669864   -98.464612  -332.834199   180.498042   \n",
       "2 -1450.933450  1928.957501   625.990570  2400.454654 -1457.796315   \n",
       "3 -3265.738566  3222.444508   291.253257 -1411.265434   968.796562   \n",
       "4  4333.526148  2619.189006  3768.893527  -412.784469  1104.611833   \n",
       "\n",
       "   feature_229  feature_230  feature_231  feature_232  feature_233  \\\n",
       "0   -82.005992  -182.494875  -154.698858   -44.773472   237.266880   \n",
       "1   -83.460820  -182.993894  -152.785112   -41.609244   241.328905   \n",
       "2   619.236134  1112.530577   807.054343    95.196853  -122.652117   \n",
       "3  -623.566475 -1164.124057  -574.162339   184.682239 -1144.981803   \n",
       "4  -991.265603  1415.507050  -923.452227 -2066.367857  -178.521065   \n",
       "\n",
       "   feature_234  feature_235  feature_236  feature_237  feature_238  \\\n",
       "0   -11.527485   -63.793977   118.925845    74.088607   113.187022   \n",
       "1    -9.520720   -63.348823   118.068359    72.076774   114.546084   \n",
       "2   -17.198895   -71.727181    -1.866234   -81.757981  -239.064685   \n",
       "3    49.973985  -369.112570   862.690530   374.757827   389.828247   \n",
       "4    69.490596  -841.153826   -75.882355  -240.844249    51.920256   \n",
       "\n",
       "   feature_239  feature_240  label  \n",
       "0   -50.485476     9.821345    0.0  \n",
       "1   -50.421345    14.769983    0.0  \n",
       "2   130.980977   -94.438973    0.0  \n",
       "3  -118.692069  -112.547679    0.0  \n",
       "4  -273.459169   524.104264    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of the Parquet file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>feature_79</th>\n",
       "      <th>feature_80</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>feature_82</th>\n",
       "      <th>feature_83</th>\n",
       "      <th>feature_84</th>\n",
       "      <th>feature_85</th>\n",
       "      <th>feature_86</th>\n",
       "      <th>feature_87</th>\n",
       "      <th>feature_88</th>\n",
       "      <th>feature_89</th>\n",
       "      <th>feature_90</th>\n",
       "      <th>feature_91</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>feature_101</th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "      <th>feature_104</th>\n",
       "      <th>feature_105</th>\n",
       "      <th>feature_106</th>\n",
       "      <th>feature_107</th>\n",
       "      <th>feature_108</th>\n",
       "      <th>feature_109</th>\n",
       "      <th>feature_110</th>\n",
       "      <th>feature_111</th>\n",
       "      <th>feature_112</th>\n",
       "      <th>feature_113</th>\n",
       "      <th>feature_114</th>\n",
       "      <th>feature_115</th>\n",
       "      <th>feature_116</th>\n",
       "      <th>feature_117</th>\n",
       "      <th>feature_118</th>\n",
       "      <th>feature_119</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>feature_130</th>\n",
       "      <th>feature_131</th>\n",
       "      <th>feature_132</th>\n",
       "      <th>feature_133</th>\n",
       "      <th>feature_134</th>\n",
       "      <th>feature_135</th>\n",
       "      <th>feature_136</th>\n",
       "      <th>feature_137</th>\n",
       "      <th>feature_138</th>\n",
       "      <th>feature_139</th>\n",
       "      <th>feature_140</th>\n",
       "      <th>feature_141</th>\n",
       "      <th>feature_142</th>\n",
       "      <th>feature_143</th>\n",
       "      <th>feature_144</th>\n",
       "      <th>feature_145</th>\n",
       "      <th>feature_146</th>\n",
       "      <th>feature_147</th>\n",
       "      <th>feature_148</th>\n",
       "      <th>feature_149</th>\n",
       "      <th>feature_150</th>\n",
       "      <th>feature_151</th>\n",
       "      <th>feature_152</th>\n",
       "      <th>feature_153</th>\n",
       "      <th>feature_154</th>\n",
       "      <th>feature_155</th>\n",
       "      <th>feature_156</th>\n",
       "      <th>feature_157</th>\n",
       "      <th>feature_158</th>\n",
       "      <th>feature_159</th>\n",
       "      <th>feature_160</th>\n",
       "      <th>feature_161</th>\n",
       "      <th>feature_162</th>\n",
       "      <th>feature_163</th>\n",
       "      <th>feature_164</th>\n",
       "      <th>feature_165</th>\n",
       "      <th>feature_166</th>\n",
       "      <th>feature_167</th>\n",
       "      <th>feature_168</th>\n",
       "      <th>feature_169</th>\n",
       "      <th>feature_170</th>\n",
       "      <th>feature_171</th>\n",
       "      <th>feature_172</th>\n",
       "      <th>feature_173</th>\n",
       "      <th>feature_174</th>\n",
       "      <th>feature_175</th>\n",
       "      <th>feature_176</th>\n",
       "      <th>feature_177</th>\n",
       "      <th>feature_178</th>\n",
       "      <th>feature_179</th>\n",
       "      <th>feature_180</th>\n",
       "      <th>feature_181</th>\n",
       "      <th>feature_182</th>\n",
       "      <th>feature_183</th>\n",
       "      <th>feature_184</th>\n",
       "      <th>feature_185</th>\n",
       "      <th>feature_186</th>\n",
       "      <th>feature_187</th>\n",
       "      <th>feature_188</th>\n",
       "      <th>feature_189</th>\n",
       "      <th>feature_190</th>\n",
       "      <th>feature_191</th>\n",
       "      <th>feature_192</th>\n",
       "      <th>feature_193</th>\n",
       "      <th>feature_194</th>\n",
       "      <th>feature_195</th>\n",
       "      <th>feature_196</th>\n",
       "      <th>feature_197</th>\n",
       "      <th>feature_198</th>\n",
       "      <th>feature_199</th>\n",
       "      <th>feature_200</th>\n",
       "      <th>feature_201</th>\n",
       "      <th>feature_202</th>\n",
       "      <th>feature_203</th>\n",
       "      <th>feature_204</th>\n",
       "      <th>feature_205</th>\n",
       "      <th>feature_206</th>\n",
       "      <th>feature_207</th>\n",
       "      <th>feature_208</th>\n",
       "      <th>feature_209</th>\n",
       "      <th>feature_210</th>\n",
       "      <th>feature_211</th>\n",
       "      <th>feature_212</th>\n",
       "      <th>feature_213</th>\n",
       "      <th>feature_214</th>\n",
       "      <th>feature_215</th>\n",
       "      <th>feature_216</th>\n",
       "      <th>feature_217</th>\n",
       "      <th>feature_218</th>\n",
       "      <th>feature_219</th>\n",
       "      <th>feature_220</th>\n",
       "      <th>feature_221</th>\n",
       "      <th>feature_222</th>\n",
       "      <th>feature_223</th>\n",
       "      <th>feature_224</th>\n",
       "      <th>feature_225</th>\n",
       "      <th>feature_226</th>\n",
       "      <th>feature_227</th>\n",
       "      <th>feature_228</th>\n",
       "      <th>feature_229</th>\n",
       "      <th>feature_230</th>\n",
       "      <th>feature_231</th>\n",
       "      <th>feature_232</th>\n",
       "      <th>feature_233</th>\n",
       "      <th>feature_234</th>\n",
       "      <th>feature_235</th>\n",
       "      <th>feature_236</th>\n",
       "      <th>feature_237</th>\n",
       "      <th>feature_238</th>\n",
       "      <th>feature_239</th>\n",
       "      <th>feature_240</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.449756</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-511.899229</td>\n",
       "      <td>-1897.728545</td>\n",
       "      <td>1892.193864</td>\n",
       "      <td>-1800.512951</td>\n",
       "      <td>1716.153147</td>\n",
       "      <td>108.966949</td>\n",
       "      <td>-1087.335503</td>\n",
       "      <td>711.265902</td>\n",
       "      <td>-414.147623</td>\n",
       "      <td>-690.902204</td>\n",
       "      <td>-295.074213</td>\n",
       "      <td>119.688772</td>\n",
       "      <td>-574.414342</td>\n",
       "      <td>18.642667</td>\n",
       "      <td>-334.981288</td>\n",
       "      <td>633.789239</td>\n",
       "      <td>256.889949</td>\n",
       "      <td>235.119026</td>\n",
       "      <td>-61.852877</td>\n",
       "      <td>-85.578348</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.927084</td>\n",
       "      <td>2553.585704</td>\n",
       "      <td>-4906.251012</td>\n",
       "      <td>2335.325503</td>\n",
       "      <td>-314.707397</td>\n",
       "      <td>-1356.950060</td>\n",
       "      <td>-898.198017</td>\n",
       "      <td>-1147.085089</td>\n",
       "      <td>418.328739</td>\n",
       "      <td>371.014384</td>\n",
       "      <td>1076.276228</td>\n",
       "      <td>193.553592</td>\n",
       "      <td>-479.117805</td>\n",
       "      <td>746.671204</td>\n",
       "      <td>-41.674053</td>\n",
       "      <td>1331.949218</td>\n",
       "      <td>-710.600821</td>\n",
       "      <td>155.939896</td>\n",
       "      <td>995.075728</td>\n",
       "      <td>-353.385971</td>\n",
       "      <td>150.517038</td>\n",
       "      <td>-0.063168</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.045745</td>\n",
       "      <td>-0.825372</td>\n",
       "      <td>1792.713630</td>\n",
       "      <td>-4293.303647</td>\n",
       "      <td>2530.837323</td>\n",
       "      <td>-1035.194496</td>\n",
       "      <td>-299.344505</td>\n",
       "      <td>-731.962749</td>\n",
       "      <td>-2155.028632</td>\n",
       "      <td>1078.906461</td>\n",
       "      <td>4.759914</td>\n",
       "      <td>1330.074696</td>\n",
       "      <td>1135.415179</td>\n",
       "      <td>-87.209520</td>\n",
       "      <td>695.512698</td>\n",
       "      <td>-68.084209</td>\n",
       "      <td>537.768010</td>\n",
       "      <td>-433.176891</td>\n",
       "      <td>77.316255</td>\n",
       "      <td>614.537327</td>\n",
       "      <td>-188.355901</td>\n",
       "      <td>152.214179</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>377.955669</td>\n",
       "      <td>-2877.793346</td>\n",
       "      <td>2271.378098</td>\n",
       "      <td>-1653.789303</td>\n",
       "      <td>1059.765109</td>\n",
       "      <td>-264.703657</td>\n",
       "      <td>-2030.153238</td>\n",
       "      <td>1164.539307</td>\n",
       "      <td>-329.725082</td>\n",
       "      <td>541.745058</td>\n",
       "      <td>1003.288816</td>\n",
       "      <td>274.968400</td>\n",
       "      <td>159.332289</td>\n",
       "      <td>-12.997391</td>\n",
       "      <td>-708.439256</td>\n",
       "      <td>457.168845</td>\n",
       "      <td>-4.379625</td>\n",
       "      <td>-362.414328</td>\n",
       "      <td>137.474288</td>\n",
       "      <td>-114.283946</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.256462</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.927084</td>\n",
       "      <td>2553.585704</td>\n",
       "      <td>-4906.251012</td>\n",
       "      <td>2335.325503</td>\n",
       "      <td>-314.707397</td>\n",
       "      <td>-1356.950060</td>\n",
       "      <td>-898.198017</td>\n",
       "      <td>-1147.085089</td>\n",
       "      <td>418.328739</td>\n",
       "      <td>371.014384</td>\n",
       "      <td>1076.276228</td>\n",
       "      <td>193.553592</td>\n",
       "      <td>-479.117805</td>\n",
       "      <td>746.671204</td>\n",
       "      <td>-41.674053</td>\n",
       "      <td>1331.949218</td>\n",
       "      <td>-710.600821</td>\n",
       "      <td>155.939896</td>\n",
       "      <td>995.075728</td>\n",
       "      <td>-353.385971</td>\n",
       "      <td>150.517038</td>\n",
       "      <td>-0.063168</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.045745</td>\n",
       "      <td>-0.825372</td>\n",
       "      <td>1792.713630</td>\n",
       "      <td>-4293.303647</td>\n",
       "      <td>2530.837323</td>\n",
       "      <td>-1035.194496</td>\n",
       "      <td>-299.344505</td>\n",
       "      <td>-731.962749</td>\n",
       "      <td>-2155.028632</td>\n",
       "      <td>1078.906461</td>\n",
       "      <td>4.759914</td>\n",
       "      <td>1330.074696</td>\n",
       "      <td>1135.415179</td>\n",
       "      <td>-87.209520</td>\n",
       "      <td>695.512698</td>\n",
       "      <td>-68.084209</td>\n",
       "      <td>537.768010</td>\n",
       "      <td>-433.176891</td>\n",
       "      <td>77.316255</td>\n",
       "      <td>614.537327</td>\n",
       "      <td>-188.355901</td>\n",
       "      <td>152.214179</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>377.955669</td>\n",
       "      <td>-2877.793346</td>\n",
       "      <td>2271.378098</td>\n",
       "      <td>-1653.789303</td>\n",
       "      <td>1059.765109</td>\n",
       "      <td>-264.703657</td>\n",
       "      <td>-2030.153238</td>\n",
       "      <td>1164.539307</td>\n",
       "      <td>-329.725082</td>\n",
       "      <td>541.745058</td>\n",
       "      <td>1003.288816</td>\n",
       "      <td>274.968400</td>\n",
       "      <td>159.332289</td>\n",
       "      <td>-12.997391</td>\n",
       "      <td>-708.439256</td>\n",
       "      <td>457.168845</td>\n",
       "      <td>-4.379625</td>\n",
       "      <td>-362.414328</td>\n",
       "      <td>137.474288</td>\n",
       "      <td>-114.283946</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2653.992815</td>\n",
       "      <td>939.153282</td>\n",
       "      <td>-509.806102</td>\n",
       "      <td>441.657659</td>\n",
       "      <td>-494.669864</td>\n",
       "      <td>-98.464612</td>\n",
       "      <td>-332.834199</td>\n",
       "      <td>180.498042</td>\n",
       "      <td>-83.460820</td>\n",
       "      <td>-182.993894</td>\n",
       "      <td>-152.785112</td>\n",
       "      <td>-41.609244</td>\n",
       "      <td>241.328905</td>\n",
       "      <td>-9.520720</td>\n",
       "      <td>-63.348823</td>\n",
       "      <td>118.068359</td>\n",
       "      <td>72.076774</td>\n",
       "      <td>114.546084</td>\n",
       "      <td>-50.421345</td>\n",
       "      <td>14.769983</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.063168</td>\n",
       "      <td>-0.045745</td>\n",
       "      <td>-0.825372</td>\n",
       "      <td>1792.713630</td>\n",
       "      <td>-4293.303647</td>\n",
       "      <td>2530.837323</td>\n",
       "      <td>-1035.194496</td>\n",
       "      <td>-299.344505</td>\n",
       "      <td>-731.962749</td>\n",
       "      <td>-2155.028632</td>\n",
       "      <td>1078.906461</td>\n",
       "      <td>4.759914</td>\n",
       "      <td>1330.074696</td>\n",
       "      <td>1135.415179</td>\n",
       "      <td>-87.209520</td>\n",
       "      <td>695.512698</td>\n",
       "      <td>-68.084209</td>\n",
       "      <td>537.768010</td>\n",
       "      <td>-433.176891</td>\n",
       "      <td>77.316255</td>\n",
       "      <td>614.537327</td>\n",
       "      <td>-188.355901</td>\n",
       "      <td>152.214179</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>377.955669</td>\n",
       "      <td>-2877.793346</td>\n",
       "      <td>2271.378098</td>\n",
       "      <td>-1653.789303</td>\n",
       "      <td>1059.765109</td>\n",
       "      <td>-264.703657</td>\n",
       "      <td>-2030.153238</td>\n",
       "      <td>1164.539307</td>\n",
       "      <td>-329.725082</td>\n",
       "      <td>541.745058</td>\n",
       "      <td>1003.288816</td>\n",
       "      <td>274.968400</td>\n",
       "      <td>159.332289</td>\n",
       "      <td>-12.997391</td>\n",
       "      <td>-708.439256</td>\n",
       "      <td>457.168845</td>\n",
       "      <td>-4.379625</td>\n",
       "      <td>-362.414328</td>\n",
       "      <td>137.474288</td>\n",
       "      <td>-114.283946</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2653.992815</td>\n",
       "      <td>939.153282</td>\n",
       "      <td>-509.806102</td>\n",
       "      <td>441.657659</td>\n",
       "      <td>-494.669864</td>\n",
       "      <td>-98.464612</td>\n",
       "      <td>-332.834199</td>\n",
       "      <td>180.498042</td>\n",
       "      <td>-83.460820</td>\n",
       "      <td>-182.993894</td>\n",
       "      <td>-152.785112</td>\n",
       "      <td>-41.609244</td>\n",
       "      <td>241.328905</td>\n",
       "      <td>-9.520720</td>\n",
       "      <td>-63.348823</td>\n",
       "      <td>118.068359</td>\n",
       "      <td>72.076774</td>\n",
       "      <td>114.546084</td>\n",
       "      <td>-50.421345</td>\n",
       "      <td>14.769983</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.040219</td>\n",
       "      <td>0.367475</td>\n",
       "      <td>-1654.273947</td>\n",
       "      <td>-495.122135</td>\n",
       "      <td>1021.132498</td>\n",
       "      <td>-1450.933450</td>\n",
       "      <td>1928.957501</td>\n",
       "      <td>625.990570</td>\n",
       "      <td>2400.454654</td>\n",
       "      <td>-1457.796315</td>\n",
       "      <td>619.236134</td>\n",
       "      <td>1112.530577</td>\n",
       "      <td>807.054343</td>\n",
       "      <td>95.196853</td>\n",
       "      <td>-122.652117</td>\n",
       "      <td>-17.198895</td>\n",
       "      <td>-71.727181</td>\n",
       "      <td>-1.866234</td>\n",
       "      <td>-81.757981</td>\n",
       "      <td>-239.064685</td>\n",
       "      <td>130.980977</td>\n",
       "      <td>-94.438973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.449756</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>377.955669</td>\n",
       "      <td>-2877.793346</td>\n",
       "      <td>2271.378098</td>\n",
       "      <td>-1653.789303</td>\n",
       "      <td>1059.765109</td>\n",
       "      <td>-264.703657</td>\n",
       "      <td>-2030.153238</td>\n",
       "      <td>1164.539307</td>\n",
       "      <td>-329.725082</td>\n",
       "      <td>541.745058</td>\n",
       "      <td>1003.288816</td>\n",
       "      <td>274.968400</td>\n",
       "      <td>159.332289</td>\n",
       "      <td>-12.997391</td>\n",
       "      <td>-708.439256</td>\n",
       "      <td>457.168845</td>\n",
       "      <td>-4.379625</td>\n",
       "      <td>-362.414328</td>\n",
       "      <td>137.474288</td>\n",
       "      <td>-114.283946</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2653.992815</td>\n",
       "      <td>939.153282</td>\n",
       "      <td>-509.806102</td>\n",
       "      <td>441.657659</td>\n",
       "      <td>-494.669864</td>\n",
       "      <td>-98.464612</td>\n",
       "      <td>-332.834199</td>\n",
       "      <td>180.498042</td>\n",
       "      <td>-83.460820</td>\n",
       "      <td>-182.993894</td>\n",
       "      <td>-152.785112</td>\n",
       "      <td>-41.609244</td>\n",
       "      <td>241.328905</td>\n",
       "      <td>-9.520720</td>\n",
       "      <td>-63.348823</td>\n",
       "      <td>118.068359</td>\n",
       "      <td>72.076774</td>\n",
       "      <td>114.546084</td>\n",
       "      <td>-50.421345</td>\n",
       "      <td>14.769983</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.040219</td>\n",
       "      <td>0.367475</td>\n",
       "      <td>-1654.273947</td>\n",
       "      <td>-495.122135</td>\n",
       "      <td>1021.132498</td>\n",
       "      <td>-1450.933450</td>\n",
       "      <td>1928.957501</td>\n",
       "      <td>625.990570</td>\n",
       "      <td>2400.454654</td>\n",
       "      <td>-1457.796315</td>\n",
       "      <td>619.236134</td>\n",
       "      <td>1112.530577</td>\n",
       "      <td>807.054343</td>\n",
       "      <td>95.196853</td>\n",
       "      <td>-122.652117</td>\n",
       "      <td>-17.198895</td>\n",
       "      <td>-71.727181</td>\n",
       "      <td>-1.866234</td>\n",
       "      <td>-81.757981</td>\n",
       "      <td>-239.064685</td>\n",
       "      <td>130.980977</td>\n",
       "      <td>-94.438973</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>774.321552</td>\n",
       "      <td>-3620.477184</td>\n",
       "      <td>3399.587477</td>\n",
       "      <td>-3265.738566</td>\n",
       "      <td>3222.444508</td>\n",
       "      <td>291.253257</td>\n",
       "      <td>-1411.265434</td>\n",
       "      <td>968.796562</td>\n",
       "      <td>-623.566475</td>\n",
       "      <td>-1164.124057</td>\n",
       "      <td>-574.162339</td>\n",
       "      <td>184.682239</td>\n",
       "      <td>-1144.981803</td>\n",
       "      <td>49.973985</td>\n",
       "      <td>-369.112570</td>\n",
       "      <td>862.690530</td>\n",
       "      <td>374.757827</td>\n",
       "      <td>389.828247</td>\n",
       "      <td>-118.692069</td>\n",
       "      <td>-112.547679</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.516713</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.677433</td>\n",
       "      <td>-1013.887299</td>\n",
       "      <td>-1306.810880</td>\n",
       "      <td>1576.879362</td>\n",
       "      <td>-1730.362375</td>\n",
       "      <td>1888.873593</td>\n",
       "      <td>287.655745</td>\n",
       "      <td>-303.580668</td>\n",
       "      <td>291.213129</td>\n",
       "      <td>-339.514773</td>\n",
       "      <td>-1138.600817</td>\n",
       "      <td>-963.117321</td>\n",
       "      <td>-72.764146</td>\n",
       "      <td>-446.154337</td>\n",
       "      <td>32.807479</td>\n",
       "      <td>274.144662</td>\n",
       "      <td>-14.479703</td>\n",
       "      <td>113.404196</td>\n",
       "      <td>360.894652</td>\n",
       "      <td>-152.953085</td>\n",
       "      <td>95.524119</td>\n",
       "      <td>-0.256462</td>\n",
       "      <td>133.0</td>\n",
       "      <td>-0.067848</td>\n",
       "      <td>-0.037538</td>\n",
       "      <td>-2586.122444</td>\n",
       "      <td>848.118498</td>\n",
       "      <td>-420.123075</td>\n",
       "      <td>326.795783</td>\n",
       "      <td>-347.256097</td>\n",
       "      <td>-44.591849</td>\n",
       "      <td>-80.509364</td>\n",
       "      <td>9.551384</td>\n",
       "      <td>6.428320</td>\n",
       "      <td>46.183642</td>\n",
       "      <td>39.418842</td>\n",
       "      <td>2.467569</td>\n",
       "      <td>-378.475560</td>\n",
       "      <td>-7.389064</td>\n",
       "      <td>128.475564</td>\n",
       "      <td>-311.480012</td>\n",
       "      <td>-199.212245</td>\n",
       "      <td>-382.762411</td>\n",
       "      <td>160.599969</td>\n",
       "      <td>-95.361173</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.047929</td>\n",
       "      <td>-2254.853445</td>\n",
       "      <td>363.563256</td>\n",
       "      <td>117.791524</td>\n",
       "      <td>-356.372051</td>\n",
       "      <td>554.798155</td>\n",
       "      <td>246.224350</td>\n",
       "      <td>1126.833343</td>\n",
       "      <td>-717.049501</td>\n",
       "      <td>345.098899</td>\n",
       "      <td>764.424331</td>\n",
       "      <td>648.789253</td>\n",
       "      <td>139.082896</td>\n",
       "      <td>-595.637608</td>\n",
       "      <td>11.603653</td>\n",
       "      <td>-51.091916</td>\n",
       "      <td>329.033295</td>\n",
       "      <td>265.275586</td>\n",
       "      <td>539.978242</td>\n",
       "      <td>-224.529041</td>\n",
       "      <td>88.602621</td>\n",
       "      <td>0.323419</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>0.624603</td>\n",
       "      <td>-2333.513215</td>\n",
       "      <td>482.056503</td>\n",
       "      <td>-16.367345</td>\n",
       "      <td>-190.581365</td>\n",
       "      <td>336.703963</td>\n",
       "      <td>183.078889</td>\n",
       "      <td>901.774797</td>\n",
       "      <td>-595.835087</td>\n",
       "      <td>297.040540</td>\n",
       "      <td>709.857510</td>\n",
       "      <td>620.251674</td>\n",
       "      <td>150.557987</td>\n",
       "      <td>-1068.140880</td>\n",
       "      <td>15.439787</td>\n",
       "      <td>161.697588</td>\n",
       "      <td>-196.871609</td>\n",
       "      <td>-69.693087</td>\n",
       "      <td>-61.658019</td>\n",
       "      <td>22.072367</td>\n",
       "      <td>-5.119332</td>\n",
       "      <td>0.130125</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.066006</td>\n",
       "      <td>0.408094</td>\n",
       "      <td>-2594.886918</td>\n",
       "      <td>856.392628</td>\n",
       "      <td>-422.935077</td>\n",
       "      <td>327.179150</td>\n",
       "      <td>-343.289108</td>\n",
       "      <td>-43.757904</td>\n",
       "      <td>-76.624347</td>\n",
       "      <td>11.753169</td>\n",
       "      <td>5.125490</td>\n",
       "      <td>49.332523</td>\n",
       "      <td>52.623456</td>\n",
       "      <td>15.897757</td>\n",
       "      <td>-337.693623</td>\n",
       "      <td>-2.546881</td>\n",
       "      <td>116.051452</td>\n",
       "      <td>-264.638371</td>\n",
       "      <td>-167.607511</td>\n",
       "      <td>-297.073758</td>\n",
       "      <td>125.329250</td>\n",
       "      <td>-53.256074</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2651.099060</td>\n",
       "      <td>935.838092</td>\n",
       "      <td>-507.312366</td>\n",
       "      <td>439.012905</td>\n",
       "      <td>-492.002428</td>\n",
       "      <td>-97.434539</td>\n",
       "      <td>-328.253610</td>\n",
       "      <td>177.183495</td>\n",
       "      <td>-82.005992</td>\n",
       "      <td>-182.494875</td>\n",
       "      <td>-154.698858</td>\n",
       "      <td>-44.773472</td>\n",
       "      <td>237.266880</td>\n",
       "      <td>-11.527485</td>\n",
       "      <td>-63.793977</td>\n",
       "      <td>118.925845</td>\n",
       "      <td>74.088607</td>\n",
       "      <td>113.187022</td>\n",
       "      <td>-50.485476</td>\n",
       "      <td>9.821345</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>-2653.992815</td>\n",
       "      <td>939.153282</td>\n",
       "      <td>-509.806102</td>\n",
       "      <td>441.657659</td>\n",
       "      <td>-494.669864</td>\n",
       "      <td>-98.464612</td>\n",
       "      <td>-332.834199</td>\n",
       "      <td>180.498042</td>\n",
       "      <td>-83.460820</td>\n",
       "      <td>-182.993894</td>\n",
       "      <td>-152.785112</td>\n",
       "      <td>-41.609244</td>\n",
       "      <td>241.328905</td>\n",
       "      <td>-9.520720</td>\n",
       "      <td>-63.348823</td>\n",
       "      <td>118.068359</td>\n",
       "      <td>72.076774</td>\n",
       "      <td>114.546084</td>\n",
       "      <td>-50.421345</td>\n",
       "      <td>14.769983</td>\n",
       "      <td>0.516713</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.040219</td>\n",
       "      <td>0.367475</td>\n",
       "      <td>-1654.273947</td>\n",
       "      <td>-495.122135</td>\n",
       "      <td>1021.132498</td>\n",
       "      <td>-1450.933450</td>\n",
       "      <td>1928.957501</td>\n",
       "      <td>625.990570</td>\n",
       "      <td>2400.454654</td>\n",
       "      <td>-1457.796315</td>\n",
       "      <td>619.236134</td>\n",
       "      <td>1112.530577</td>\n",
       "      <td>807.054343</td>\n",
       "      <td>95.196853</td>\n",
       "      <td>-122.652117</td>\n",
       "      <td>-17.198895</td>\n",
       "      <td>-71.727181</td>\n",
       "      <td>-1.866234</td>\n",
       "      <td>-81.757981</td>\n",
       "      <td>-239.064685</td>\n",
       "      <td>130.980977</td>\n",
       "      <td>-94.438973</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>774.321552</td>\n",
       "      <td>-3620.477184</td>\n",
       "      <td>3399.587477</td>\n",
       "      <td>-3265.738566</td>\n",
       "      <td>3222.444508</td>\n",
       "      <td>291.253257</td>\n",
       "      <td>-1411.265434</td>\n",
       "      <td>968.796562</td>\n",
       "      <td>-623.566475</td>\n",
       "      <td>-1164.124057</td>\n",
       "      <td>-574.162339</td>\n",
       "      <td>184.682239</td>\n",
       "      <td>-1144.981803</td>\n",
       "      <td>49.973985</td>\n",
       "      <td>-369.112570</td>\n",
       "      <td>862.690530</td>\n",
       "      <td>374.757827</td>\n",
       "      <td>389.828247</td>\n",
       "      <td>-118.692069</td>\n",
       "      <td>-112.547679</td>\n",
       "      <td>-0.449756</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>-0.935017</td>\n",
       "      <td>16629.491533</td>\n",
       "      <td>-9161.244843</td>\n",
       "      <td>-6259.082448</td>\n",
       "      <td>4333.526148</td>\n",
       "      <td>2619.189006</td>\n",
       "      <td>3768.893527</td>\n",
       "      <td>-412.784469</td>\n",
       "      <td>1104.611833</td>\n",
       "      <td>-991.265603</td>\n",
       "      <td>1415.507050</td>\n",
       "      <td>-923.452227</td>\n",
       "      <td>-2066.367857</td>\n",
       "      <td>-178.521065</td>\n",
       "      <td>69.490596</td>\n",
       "      <td>-841.153826</td>\n",
       "      <td>-75.882355</td>\n",
       "      <td>-240.844249</td>\n",
       "      <td>51.920256</td>\n",
       "      <td>-273.459169</td>\n",
       "      <td>524.104264</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_3  feature_4    feature_5    feature_6    feature_7  \\\n",
       "0  -0.449756  -0.071532  -0.935017  -511.899229 -1897.728545  1892.193864   \n",
       "1  -0.256462  -0.067848  -0.927084  2553.585704 -4906.251012  2335.325503   \n",
       "2  -0.063168  -0.045745  -0.825372  1792.713630 -4293.303647  2530.837323   \n",
       "3  -0.449756  -0.071532  -0.935017   377.955669 -2877.793346  2271.378098   \n",
       "4   0.516713  -0.060481  -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "\n",
       "     feature_8    feature_9  feature_10   feature_11   feature_12  feature_13  \\\n",
       "0 -1800.512951  1716.153147  108.966949 -1087.335503   711.265902 -414.147623   \n",
       "1  -314.707397 -1356.950060 -898.198017 -1147.085089   418.328739  371.014384   \n",
       "2 -1035.194496  -299.344505 -731.962749 -2155.028632  1078.906461    4.759914   \n",
       "3 -1653.789303  1059.765109 -264.703657 -2030.153238  1164.539307 -329.725082   \n",
       "4 -1730.362375  1888.873593  287.655745  -303.580668   291.213129 -339.514773   \n",
       "\n",
       "    feature_14   feature_15  feature_16  feature_17  feature_18   feature_19  \\\n",
       "0  -690.902204  -295.074213  119.688772 -574.414342   18.642667  -334.981288   \n",
       "1  1076.276228   193.553592 -479.117805  746.671204  -41.674053  1331.949218   \n",
       "2  1330.074696  1135.415179  -87.209520  695.512698  -68.084209   537.768010   \n",
       "3   541.745058  1003.288816  274.968400  159.332289  -12.997391  -708.439256   \n",
       "4 -1138.600817  -963.117321  -72.764146 -446.154337   32.807479   274.144662   \n",
       "\n",
       "   feature_20  feature_21  feature_22  feature_23  feature_24  feature_25  \\\n",
       "0  633.789239  256.889949  235.119026  -61.852877  -85.578348   -0.256462   \n",
       "1 -710.600821  155.939896  995.075728 -353.385971  150.517038   -0.063168   \n",
       "2 -433.176891   77.316255  614.537327 -188.355901  152.214179   -0.449756   \n",
       "3  457.168845   -4.379625 -362.414328  137.474288 -114.283946    0.516713   \n",
       "4  -14.479703  113.404196  360.894652 -152.953085   95.524119   -0.256462   \n",
       "\n",
       "   feature_26  feature_27  feature_28   feature_29   feature_30   feature_31  \\\n",
       "0        36.0   -0.067848   -0.927084  2553.585704 -4906.251012  2335.325503   \n",
       "1        36.0   -0.045745   -0.825372  1792.713630 -4293.303647  2530.837323   \n",
       "2        36.0   -0.071532   -0.935017   377.955669 -2877.793346  2271.378098   \n",
       "3        36.0   -0.060481   -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "4       133.0   -0.067848   -0.037538 -2586.122444   848.118498  -420.123075   \n",
       "\n",
       "    feature_32   feature_33  feature_34   feature_35   feature_36  feature_37  \\\n",
       "0  -314.707397 -1356.950060 -898.198017 -1147.085089   418.328739  371.014384   \n",
       "1 -1035.194496  -299.344505 -731.962749 -2155.028632  1078.906461    4.759914   \n",
       "2 -1653.789303  1059.765109 -264.703657 -2030.153238  1164.539307 -329.725082   \n",
       "3 -1730.362375  1888.873593  287.655745  -303.580668   291.213129 -339.514773   \n",
       "4   326.795783  -347.256097  -44.591849   -80.509364     9.551384    6.428320   \n",
       "\n",
       "    feature_38   feature_39  feature_40  feature_41  feature_42   feature_43  \\\n",
       "0  1076.276228   193.553592 -479.117805  746.671204  -41.674053  1331.949218   \n",
       "1  1330.074696  1135.415179  -87.209520  695.512698  -68.084209   537.768010   \n",
       "2   541.745058  1003.288816  274.968400  159.332289  -12.997391  -708.439256   \n",
       "3 -1138.600817  -963.117321  -72.764146 -446.154337   32.807479   274.144662   \n",
       "4    46.183642    39.418842    2.467569 -378.475560   -7.389064   128.475564   \n",
       "\n",
       "   feature_44  feature_45  feature_46  feature_47  feature_48  feature_49  \\\n",
       "0 -710.600821  155.939896  995.075728 -353.385971  150.517038   -0.063168   \n",
       "1 -433.176891   77.316255  614.537327 -188.355901  152.214179   -0.449756   \n",
       "2  457.168845   -4.379625 -362.414328  137.474288 -114.283946    0.516713   \n",
       "3  -14.479703  113.404196  360.894652 -152.953085   95.524119   -0.256462   \n",
       "4 -311.480012 -199.212245 -382.762411  160.599969  -95.361173    0.516713   \n",
       "\n",
       "   feature_50  feature_51  feature_52   feature_53   feature_54   feature_55  \\\n",
       "0        36.0   -0.045745   -0.825372  1792.713630 -4293.303647  2530.837323   \n",
       "1        36.0   -0.071532   -0.935017   377.955669 -2877.793346  2271.378098   \n",
       "2        36.0   -0.060481   -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "3       133.0   -0.067848   -0.037538 -2586.122444   848.118498  -420.123075   \n",
       "4        60.0   -0.038377    0.047929 -2254.853445   363.563256   117.791524   \n",
       "\n",
       "    feature_56   feature_57  feature_58   feature_59   feature_60  feature_61  \\\n",
       "0 -1035.194496  -299.344505 -731.962749 -2155.028632  1078.906461    4.759914   \n",
       "1 -1653.789303  1059.765109 -264.703657 -2030.153238  1164.539307 -329.725082   \n",
       "2 -1730.362375  1888.873593  287.655745  -303.580668   291.213129 -339.514773   \n",
       "3   326.795783  -347.256097  -44.591849   -80.509364     9.551384    6.428320   \n",
       "4  -356.372051   554.798155  246.224350  1126.833343  -717.049501  345.098899   \n",
       "\n",
       "    feature_62   feature_63  feature_64  feature_65  feature_66  feature_67  \\\n",
       "0  1330.074696  1135.415179  -87.209520  695.512698  -68.084209  537.768010   \n",
       "1   541.745058  1003.288816  274.968400  159.332289  -12.997391 -708.439256   \n",
       "2 -1138.600817  -963.117321  -72.764146 -446.154337   32.807479  274.144662   \n",
       "3    46.183642    39.418842    2.467569 -378.475560   -7.389064  128.475564   \n",
       "4   764.424331   648.789253  139.082896 -595.637608   11.603653  -51.091916   \n",
       "\n",
       "   feature_68  feature_69  feature_70  feature_71  feature_72  feature_73  \\\n",
       "0 -433.176891   77.316255  614.537327 -188.355901  152.214179   -0.449756   \n",
       "1  457.168845   -4.379625 -362.414328  137.474288 -114.283946    0.516713   \n",
       "2  -14.479703  113.404196  360.894652 -152.953085   95.524119   -0.256462   \n",
       "3 -311.480012 -199.212245 -382.762411  160.599969  -95.361173    0.516713   \n",
       "4  329.033295  265.275586  539.978242 -224.529041   88.602621    0.323419   \n",
       "\n",
       "   feature_74  feature_75  feature_76   feature_77   feature_78   feature_79  \\\n",
       "0        36.0   -0.071532   -0.935017   377.955669 -2877.793346  2271.378098   \n",
       "1        36.0   -0.060481   -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "2       133.0   -0.067848   -0.037538 -2586.122444   848.118498  -420.123075   \n",
       "3        60.0   -0.038377    0.047929 -2254.853445   363.563256   117.791524   \n",
       "4        50.0   -0.060481    0.624603 -2333.513215   482.056503   -16.367345   \n",
       "\n",
       "    feature_80   feature_81  feature_82   feature_83   feature_84  feature_85  \\\n",
       "0 -1653.789303  1059.765109 -264.703657 -2030.153238  1164.539307 -329.725082   \n",
       "1 -1730.362375  1888.873593  287.655745  -303.580668   291.213129 -339.514773   \n",
       "2   326.795783  -347.256097  -44.591849   -80.509364     9.551384    6.428320   \n",
       "3  -356.372051   554.798155  246.224350  1126.833343  -717.049501  345.098899   \n",
       "4  -190.581365   336.703963  183.078889   901.774797  -595.835087  297.040540   \n",
       "\n",
       "    feature_86   feature_87  feature_88   feature_89  feature_90  feature_91  \\\n",
       "0   541.745058  1003.288816  274.968400   159.332289  -12.997391 -708.439256   \n",
       "1 -1138.600817  -963.117321  -72.764146  -446.154337   32.807479  274.144662   \n",
       "2    46.183642    39.418842    2.467569  -378.475560   -7.389064  128.475564   \n",
       "3   764.424331   648.789253  139.082896  -595.637608   11.603653  -51.091916   \n",
       "4   709.857510   620.251674  150.557987 -1068.140880   15.439787  161.697588   \n",
       "\n",
       "   feature_92  feature_93  feature_94  feature_95  feature_96  feature_97  \\\n",
       "0  457.168845   -4.379625 -362.414328  137.474288 -114.283946    0.516713   \n",
       "1  -14.479703  113.404196  360.894652 -152.953085   95.524119   -0.256462   \n",
       "2 -311.480012 -199.212245 -382.762411  160.599969  -95.361173    0.516713   \n",
       "3  329.033295  265.275586  539.978242 -224.529041   88.602621    0.323419   \n",
       "4 -196.871609  -69.693087  -61.658019   22.072367   -5.119332    0.130125   \n",
       "\n",
       "   feature_98  feature_99  feature_100  feature_101  feature_102  feature_103  \\\n",
       "0        36.0   -0.060481    -0.677433 -1013.887299 -1306.810880  1576.879362   \n",
       "1       133.0   -0.067848    -0.037538 -2586.122444   848.118498  -420.123075   \n",
       "2        60.0   -0.038377     0.047929 -2254.853445   363.563256   117.791524   \n",
       "3        50.0   -0.060481     0.624603 -2333.513215   482.056503   -16.367345   \n",
       "4        60.0   -0.066006     0.408094 -2594.886918   856.392628  -422.935077   \n",
       "\n",
       "   feature_104  feature_105  feature_106  feature_107  feature_108  \\\n",
       "0 -1730.362375  1888.873593   287.655745  -303.580668   291.213129   \n",
       "1   326.795783  -347.256097   -44.591849   -80.509364     9.551384   \n",
       "2  -356.372051   554.798155   246.224350  1126.833343  -717.049501   \n",
       "3  -190.581365   336.703963   183.078889   901.774797  -595.835087   \n",
       "4   327.179150  -343.289108   -43.757904   -76.624347    11.753169   \n",
       "\n",
       "   feature_109  feature_110  feature_111  feature_112  feature_113  \\\n",
       "0  -339.514773 -1138.600817  -963.117321   -72.764146  -446.154337   \n",
       "1     6.428320    46.183642    39.418842     2.467569  -378.475560   \n",
       "2   345.098899   764.424331   648.789253   139.082896  -595.637608   \n",
       "3   297.040540   709.857510   620.251674   150.557987 -1068.140880   \n",
       "4     5.125490    49.332523    52.623456    15.897757  -337.693623   \n",
       "\n",
       "   feature_114  feature_115  feature_116  feature_117  feature_118  \\\n",
       "0    32.807479   274.144662   -14.479703   113.404196   360.894652   \n",
       "1    -7.389064   128.475564  -311.480012  -199.212245  -382.762411   \n",
       "2    11.603653   -51.091916   329.033295   265.275586   539.978242   \n",
       "3    15.439787   161.697588  -196.871609   -69.693087   -61.658019   \n",
       "4    -2.546881   116.051452  -264.638371  -167.607511  -297.073758   \n",
       "\n",
       "   feature_119  feature_120  feature_121  feature_122  feature_123  \\\n",
       "0  -152.953085    95.524119    -0.256462        133.0    -0.067848   \n",
       "1   160.599969   -95.361173     0.516713         60.0    -0.038377   \n",
       "2  -224.529041    88.602621     0.323419         50.0    -0.060481   \n",
       "3    22.072367    -5.119332     0.130125         60.0    -0.066006   \n",
       "4   125.329250   -53.256074    -0.449756        102.0    -0.071532   \n",
       "\n",
       "   feature_124  feature_125  feature_126  feature_127  feature_128  \\\n",
       "0    -0.037538 -2586.122444   848.118498  -420.123075   326.795783   \n",
       "1     0.047929 -2254.853445   363.563256   117.791524  -356.372051   \n",
       "2     0.624603 -2333.513215   482.056503   -16.367345  -190.581365   \n",
       "3     0.408094 -2594.886918   856.392628  -422.935077   327.179150   \n",
       "4    -0.935017 -2651.099060   935.838092  -507.312366   439.012905   \n",
       "\n",
       "   feature_129  feature_130  feature_131  feature_132  feature_133  \\\n",
       "0  -347.256097   -44.591849   -80.509364     9.551384     6.428320   \n",
       "1   554.798155   246.224350  1126.833343  -717.049501   345.098899   \n",
       "2   336.703963   183.078889   901.774797  -595.835087   297.040540   \n",
       "3  -343.289108   -43.757904   -76.624347    11.753169     5.125490   \n",
       "4  -492.002428   -97.434539  -328.253610   177.183495   -82.005992   \n",
       "\n",
       "   feature_134  feature_135  feature_136  feature_137  feature_138  \\\n",
       "0    46.183642    39.418842     2.467569  -378.475560    -7.389064   \n",
       "1   764.424331   648.789253   139.082896  -595.637608    11.603653   \n",
       "2   709.857510   620.251674   150.557987 -1068.140880    15.439787   \n",
       "3    49.332523    52.623456    15.897757  -337.693623    -2.546881   \n",
       "4  -182.494875  -154.698858   -44.773472   237.266880   -11.527485   \n",
       "\n",
       "   feature_139  feature_140  feature_141  feature_142  feature_143  \\\n",
       "0   128.475564  -311.480012  -199.212245  -382.762411   160.599969   \n",
       "1   -51.091916   329.033295   265.275586   539.978242  -224.529041   \n",
       "2   161.697588  -196.871609   -69.693087   -61.658019    22.072367   \n",
       "3   116.051452  -264.638371  -167.607511  -297.073758   125.329250   \n",
       "4   -63.793977   118.925845    74.088607   113.187022   -50.485476   \n",
       "\n",
       "   feature_144  feature_145  feature_146  feature_147  feature_148  \\\n",
       "0   -95.361173     0.516713         60.0    -0.038377     0.047929   \n",
       "1    88.602621     0.323419         50.0    -0.060481     0.624603   \n",
       "2    -5.119332     0.130125         60.0    -0.066006     0.408094   \n",
       "3   -53.256074    -0.449756        102.0    -0.071532    -0.935017   \n",
       "4     9.821345    -0.449756         56.0    -0.071532    -0.935017   \n",
       "\n",
       "   feature_149  feature_150  feature_151  feature_152  feature_153  \\\n",
       "0 -2254.853445   363.563256   117.791524  -356.372051   554.798155   \n",
       "1 -2333.513215   482.056503   -16.367345  -190.581365   336.703963   \n",
       "2 -2594.886918   856.392628  -422.935077   327.179150  -343.289108   \n",
       "3 -2651.099060   935.838092  -507.312366   439.012905  -492.002428   \n",
       "4 -2653.992815   939.153282  -509.806102   441.657659  -494.669864   \n",
       "\n",
       "   feature_154  feature_155  feature_156  feature_157  feature_158  \\\n",
       "0   246.224350  1126.833343  -717.049501   345.098899   764.424331   \n",
       "1   183.078889   901.774797  -595.835087   297.040540   709.857510   \n",
       "2   -43.757904   -76.624347    11.753169     5.125490    49.332523   \n",
       "3   -97.434539  -328.253610   177.183495   -82.005992  -182.494875   \n",
       "4   -98.464612  -332.834199   180.498042   -83.460820  -182.993894   \n",
       "\n",
       "   feature_159  feature_160  feature_161  feature_162  feature_163  \\\n",
       "0   648.789253   139.082896  -595.637608    11.603653   -51.091916   \n",
       "1   620.251674   150.557987 -1068.140880    15.439787   161.697588   \n",
       "2    52.623456    15.897757  -337.693623    -2.546881   116.051452   \n",
       "3  -154.698858   -44.773472   237.266880   -11.527485   -63.793977   \n",
       "4  -152.785112   -41.609244   241.328905    -9.520720   -63.348823   \n",
       "\n",
       "   feature_164  feature_165  feature_166  feature_167  feature_168  \\\n",
       "0   329.033295   265.275586   539.978242  -224.529041    88.602621   \n",
       "1  -196.871609   -69.693087   -61.658019    22.072367    -5.119332   \n",
       "2  -264.638371  -167.607511  -297.073758   125.329250   -53.256074   \n",
       "3   118.925845    74.088607   113.187022   -50.485476     9.821345   \n",
       "4   118.068359    72.076774   114.546084   -50.421345    14.769983   \n",
       "\n",
       "   feature_169  feature_170  feature_171  feature_172  feature_173  \\\n",
       "0     0.323419         50.0    -0.060481     0.624603 -2333.513215   \n",
       "1     0.130125         60.0    -0.066006     0.408094 -2594.886918   \n",
       "2    -0.449756        102.0    -0.071532    -0.935017 -2651.099060   \n",
       "3    -0.449756         56.0    -0.071532    -0.935017 -2653.992815   \n",
       "4     0.516713         56.0    -0.040219     0.367475 -1654.273947   \n",
       "\n",
       "   feature_174  feature_175  feature_176  feature_177  feature_178  \\\n",
       "0   482.056503   -16.367345  -190.581365   336.703963   183.078889   \n",
       "1   856.392628  -422.935077   327.179150  -343.289108   -43.757904   \n",
       "2   935.838092  -507.312366   439.012905  -492.002428   -97.434539   \n",
       "3   939.153282  -509.806102   441.657659  -494.669864   -98.464612   \n",
       "4  -495.122135  1021.132498 -1450.933450  1928.957501   625.990570   \n",
       "\n",
       "   feature_179  feature_180  feature_181  feature_182  feature_183  \\\n",
       "0   901.774797  -595.835087   297.040540   709.857510   620.251674   \n",
       "1   -76.624347    11.753169     5.125490    49.332523    52.623456   \n",
       "2  -328.253610   177.183495   -82.005992  -182.494875  -154.698858   \n",
       "3  -332.834199   180.498042   -83.460820  -182.993894  -152.785112   \n",
       "4  2400.454654 -1457.796315   619.236134  1112.530577   807.054343   \n",
       "\n",
       "   feature_184  feature_185  feature_186  feature_187  feature_188  \\\n",
       "0   150.557987 -1068.140880    15.439787   161.697588  -196.871609   \n",
       "1    15.897757  -337.693623    -2.546881   116.051452  -264.638371   \n",
       "2   -44.773472   237.266880   -11.527485   -63.793977   118.925845   \n",
       "3   -41.609244   241.328905    -9.520720   -63.348823   118.068359   \n",
       "4    95.196853  -122.652117   -17.198895   -71.727181    -1.866234   \n",
       "\n",
       "   feature_189  feature_190  feature_191  feature_192  feature_193  \\\n",
       "0   -69.693087   -61.658019    22.072367    -5.119332     0.130125   \n",
       "1  -167.607511  -297.073758   125.329250   -53.256074    -0.449756   \n",
       "2    74.088607   113.187022   -50.485476     9.821345    -0.449756   \n",
       "3    72.076774   114.546084   -50.421345    14.769983     0.516713   \n",
       "4   -81.757981  -239.064685   130.980977   -94.438973    -0.449756   \n",
       "\n",
       "   feature_194  feature_195  feature_196  feature_197  feature_198  \\\n",
       "0         60.0    -0.066006     0.408094 -2594.886918   856.392628   \n",
       "1        102.0    -0.071532    -0.935017 -2651.099060   935.838092   \n",
       "2         56.0    -0.071532    -0.935017 -2653.992815   939.153282   \n",
       "3         56.0    -0.040219     0.367475 -1654.273947  -495.122135   \n",
       "4         56.0    -0.071532    -0.935017   774.321552 -3620.477184   \n",
       "\n",
       "   feature_199  feature_200  feature_201  feature_202  feature_203  \\\n",
       "0  -422.935077   327.179150  -343.289108   -43.757904   -76.624347   \n",
       "1  -507.312366   439.012905  -492.002428   -97.434539  -328.253610   \n",
       "2  -509.806102   441.657659  -494.669864   -98.464612  -332.834199   \n",
       "3  1021.132498 -1450.933450  1928.957501   625.990570  2400.454654   \n",
       "4  3399.587477 -3265.738566  3222.444508   291.253257 -1411.265434   \n",
       "\n",
       "   feature_204  feature_205  feature_206  feature_207  feature_208  \\\n",
       "0    11.753169     5.125490    49.332523    52.623456    15.897757   \n",
       "1   177.183495   -82.005992  -182.494875  -154.698858   -44.773472   \n",
       "2   180.498042   -83.460820  -182.993894  -152.785112   -41.609244   \n",
       "3 -1457.796315   619.236134  1112.530577   807.054343    95.196853   \n",
       "4   968.796562  -623.566475 -1164.124057  -574.162339   184.682239   \n",
       "\n",
       "   feature_209  feature_210  feature_211  feature_212  feature_213  \\\n",
       "0  -337.693623    -2.546881   116.051452  -264.638371  -167.607511   \n",
       "1   237.266880   -11.527485   -63.793977   118.925845    74.088607   \n",
       "2   241.328905    -9.520720   -63.348823   118.068359    72.076774   \n",
       "3  -122.652117   -17.198895   -71.727181    -1.866234   -81.757981   \n",
       "4 -1144.981803    49.973985  -369.112570   862.690530   374.757827   \n",
       "\n",
       "   feature_214  feature_215  feature_216  feature_217  feature_218  \\\n",
       "0  -297.073758   125.329250   -53.256074    -0.449756        102.0   \n",
       "1   113.187022   -50.485476     9.821345    -0.449756         56.0   \n",
       "2   114.546084   -50.421345    14.769983     0.516713         56.0   \n",
       "3  -239.064685   130.980977   -94.438973    -0.449756         56.0   \n",
       "4   389.828247  -118.692069  -112.547679    -0.449756         56.0   \n",
       "\n",
       "   feature_219  feature_220   feature_221  feature_222  feature_223  \\\n",
       "0    -0.071532    -0.935017  -2651.099060   935.838092  -507.312366   \n",
       "1    -0.071532    -0.935017  -2653.992815   939.153282  -509.806102   \n",
       "2    -0.040219     0.367475  -1654.273947  -495.122135  1021.132498   \n",
       "3    -0.071532    -0.935017    774.321552 -3620.477184  3399.587477   \n",
       "4    -0.071532    -0.935017  16629.491533 -9161.244843 -6259.082448   \n",
       "\n",
       "   feature_224  feature_225  feature_226  feature_227  feature_228  \\\n",
       "0   439.012905  -492.002428   -97.434539  -328.253610   177.183495   \n",
       "1   441.657659  -494.669864   -98.464612  -332.834199   180.498042   \n",
       "2 -1450.933450  1928.957501   625.990570  2400.454654 -1457.796315   \n",
       "3 -3265.738566  3222.444508   291.253257 -1411.265434   968.796562   \n",
       "4  4333.526148  2619.189006  3768.893527  -412.784469  1104.611833   \n",
       "\n",
       "   feature_229  feature_230  feature_231  feature_232  feature_233  \\\n",
       "0   -82.005992  -182.494875  -154.698858   -44.773472   237.266880   \n",
       "1   -83.460820  -182.993894  -152.785112   -41.609244   241.328905   \n",
       "2   619.236134  1112.530577   807.054343    95.196853  -122.652117   \n",
       "3  -623.566475 -1164.124057  -574.162339   184.682239 -1144.981803   \n",
       "4  -991.265603  1415.507050  -923.452227 -2066.367857  -178.521065   \n",
       "\n",
       "   feature_234  feature_235  feature_236  feature_237  feature_238  \\\n",
       "0   -11.527485   -63.793977   118.925845    74.088607   113.187022   \n",
       "1    -9.520720   -63.348823   118.068359    72.076774   114.546084   \n",
       "2   -17.198895   -71.727181    -1.866234   -81.757981  -239.064685   \n",
       "3    49.973985  -369.112570   862.690530   374.757827   389.828247   \n",
       "4    69.490596  -841.153826   -75.882355  -240.844249    51.920256   \n",
       "\n",
       "   feature_239  feature_240  label  \n",
       "0   -50.485476     9.821345    0.0  \n",
       "1   -50.421345    14.769983    0.0  \n",
       "2   130.980977   -94.438973    0.0  \n",
       "3  -118.692069  -112.547679    0.0  \n",
       "4  -273.459169   524.104264    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the first 5 lines of the CSV file\n",
    "# df_csv = pd.read_csv(csv_output_path)\n",
    "# print(\"First 5 lines of the CSV file:\")\n",
    "# display(df_csv.head())\n",
    "\n",
    "# # Display the first 5 lines of the Parquet file\n",
    "# df_parquet = pd.read_parquet(parquet_output_path)\n",
    "# print(\"First 5 lines of the Parquet file:\")\n",
    "# display(df_parquet.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695caa4a-fe6a-4a8f-afe0-69b5342da7fd",
   "metadata": {},
   "source": [
    "### <font color = grey> Final Conversions\n",
    "> #1 convert all -ve values to +ve (ie abs)\n",
    ">\n",
    "> #2 normalize all values>1  except label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ea61c8-9336-4b4c-b6f6-cec5b05976b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved to CSV at /home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS/c20240705_TODS_sample_normalized_v1.01.csv\n",
      "Modified data saved to Parquet at /home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS/c20240705_TODS_sample_normalized_v1.01.parquet\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# # Load the Parquet file\n",
    "# parquet_output_path = f'{output_dir}/c20240705_TODS_sample_v1.00.parquet'\n",
    "# df_parquet = pd.read_parquet(parquet_output_path)\n",
    "\n",
    "# # Get the feature columns excluding the label\n",
    "# feature_columns = [col for col in df_parquet.columns if col != 'label']\n",
    "\n",
    "# # Process the data in steps\n",
    "# # 1. Convert all negative values to positive\n",
    "# df_parquet[feature_columns] = df_parquet[feature_columns].applymap(lambda x: abs(x))\n",
    "\n",
    "# # 2. Normalize/scale values greater than 1\n",
    "# scaler = MinMaxScaler()\n",
    "# for col in feature_columns:\n",
    "#     if df_parquet[col].max() > 1:\n",
    "#         df_parquet[col] = scaler.fit_transform(df_parquet[[col]])\n",
    "\n",
    "# # Save the modified data to CSV and Parquet\n",
    "# csv_output_path = f'{output_dir}/c20240705_TODS_sample_normalized_v1.01.csv'\n",
    "# df_parquet.to_csv(csv_output_path, index=False)\n",
    "\n",
    "# parquet_output_path = f'{output_dir}/c20240705_TODS_sample_normalized_v1.01.parquet'\n",
    "# df_parquet.to_parquet(parquet_output_path, index=False)\n",
    "\n",
    "# print(f\"Modified data saved to CSV at {csv_output_path}\")\n",
    "# print(f\"Modified data saved to Parquet at {parquet_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ab753-2b1c-41f9-ad5a-444b7e37dfbc",
   "metadata": {},
   "source": [
    "### <font color = grey> Review generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8aa8a11-aa46-4237-ba7c-4b17e9272810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # output_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/10.20240705_StartOver_NewFeatures/02.Full_Train_Test'tput_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS'\n",
    "# output_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS'\n",
    "\n",
    "# # Define the file paths\n",
    "# csv_output_path = f'{output_dir}/c20240705_TODS_sample_normalized_v1.01.csv'\n",
    "# parquet_output_path = f'{output_dir}/c20240705_TODS_sample_normalized_v1.01.parquet'\n",
    "\n",
    "# # Display the first 5 lines of the CSV file\n",
    "# df_csv = pd.read_csv(csv_output_path)\n",
    "# print(f\"First 5 lines of the CSV file ({csv_output_path}):\")\n",
    "# display(df_csv.head())\n",
    "\n",
    "# # Get the number of columns in the CSV file\n",
    "# num_columns_csv = df_csv.shape[1]\n",
    "# print(f\"Total number of columns in the CSV file: {num_columns_csv}\")\n",
    "\n",
    "# # Display the first 5 lines of the Parquet file\n",
    "# df_parquet = pd.read_parquet(parquet_output_path)\n",
    "# print(f\"First 5 lines of the Parquet file ({parquet_output_path}):\")\n",
    "# display(df_parquet.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42d75ac-1a6a-4381-8e9d-929a1f78166d",
   "metadata": {},
   "source": [
    "___\n",
    "# <font color = tomato> TODS --> recreate Colab File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b968edc-fd86-4eab-9fa1-1ff0dfb2615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from d3m import index\n",
    "from d3m.metadata.base import ArgumentType\n",
    "from d3m.metadata.pipeline import Pipeline, PrimitiveStep\n",
    "from axolotl.backend.simple import SimpleRunner\n",
    "from tods import generate_dataset, generate_problem\n",
    "from tods.searcher import BruteForceSearch\n",
    "from tods import generate_dataset, load_pipeline, evaluate_pipeline\n",
    "from tods.sk_interface.detection_algorithm.DeepLog_skinterface import DeepLogSKI\n",
    "from tods.sk_interface.detection_algorithm.Telemanom_skinterface import TelemanomSKI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b938e6-3da7-4e20-a0f8-28abf503312e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f988fa9-da41-4b72-9480-2c6bdb464bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yahoo = pd.read_csv('/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS/yahoo_sub_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08d16e5-ad2a-4156-ba75-e2679af1782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1400, 7)\n",
      "First 5 rows:\n",
      "    timestamp  value_0   value_1   value_2  value_3  value_4  anomaly\n",
      "0          1    12183  0.000000  3.716667        5     2109        0\n",
      "1          2    12715  0.091758  3.610833       60     3229        0\n",
      "2          3    12736  0.172297  3.481389       88     3637        0\n",
      "3          4    12716  0.226219  3.380278       84     1982        0\n",
      "4          5    12739  0.176358  3.193333      111     2751        0\n"
     ]
    }
   ],
   "source": [
    "print(\"shape:\", data_yahoo.shape)\n",
    "print(\"First 5 rows:\\n\", data_yahoo[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a85c8-1a0c-4bc5-9feb-bbf0323c95bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde61fc4-710f-49cc-9756-57da42960585",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_dataset_problem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-41605f2bb9c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtods\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mschemas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mschemas_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_dataset_problem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# table_path = 'datasets/yahoo_sub_5.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'generate_dataset_problem'"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# from tods import schemas as schemas_utils\n",
    "# from tods.utils import generate_dataset_problem, evaluate_pipeline\n",
    "\n",
    "# # table_path = 'datasets/yahoo_sub_5.csv'\n",
    "\n",
    "# table_path = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS/yahoo_sub_5.csv'\n",
    "# # table_path = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS/c20240705_TODS_sample_normalized_v1.01.csv'\n",
    "# target_index = 240 # what column is the target\n",
    "# metric = 'F1_MACRO' # F1 on both label 0 and 1\n",
    "\n",
    "# # Read data and generate dataset and problem\n",
    "# df = pd.read_csv(table_path)\n",
    "# dataset, problem_description = generate_dataset_problem(df, target_index)\n",
    "\n",
    "# # Load the default pipeline\n",
    "# pipeline = schemas_utils.load_default_pipeline()\n",
    "\n",
    "# # Run the pipeline\n",
    "# pipeline_result = evaluate_pipeline(dataset, pipeline, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879d46d0-0a3e-44fa-af54-3c6ddda2a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/11.20240705_TODS/yahoo_sub_5.csv'\n",
    "target_index = 240 # column of the target label\n",
    "time_limit = 30 # How many seconds you wanna search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f73d1b6-d1f3-41c6-a224-553d38ea6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'F1_MACRO' # F1 on both label 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237c965c-de7b-40de-aee9-95af0b7d8163",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input dataframe does not contains targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5c566321d37e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mproblem_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_problem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/efs-w210-capstone-ebs/08.GIT_Repos_Anomaly_Detection/tods/src/tods/tods/utils.py\u001b[0m in \u001b[0;36mgenerate_problem\u001b[0;34m(dataset, metric)\u001b[0m\n\u001b[1;32m    144\u001b[0m     problem_description = data_problem.generate_problem_description(dataset=dataset,\n\u001b[1;32m    145\u001b[0m                                                                     \u001b[0mtask_keywords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTaskKeyword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mANOMALY_DETECTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                                                                     performance_metrics=performance_metrics)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mproblem_description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/20240705_TODS_StartOver/lib/python3.6/site-packages/axolotl/utils/data_problem.py\u001b[0m in \u001b[0;36mgenerate_problem_description\u001b[0;34m(dataset, task, task_keywords, performance_metrics)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_column_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input dataframe does not contains targets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     inputs = {\n",
      "\u001b[0;31mValueError\u001b[0m: Input dataframe does not contains targets"
     ]
    }
   ],
   "source": [
    "# Read data and generate dataset and problem\n",
    "df = pd.read_csv(table_path)\n",
    "dataset = generate_dataset(df, target_index=target_index)\n",
    "problem_description = generate_problem(dataset, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011bc600-8994-459f-972c-9ff6f56c7c01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19dbbee7-c360-4140-80ee-6fad958a9f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_dataset_problem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1014668ad843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_dataset_problem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'generate_dataset_problem'"
     ]
    }
   ],
   "source": [
    "from tods.utils import generate_dataset_problem, evaluate_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf9a68-600f-4ea0-80ef-d6a1c14d19cf",
   "metadata": {},
   "source": [
    "___\n",
    "## <font color = grey> GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136c729-8d3d-4f43-979f-5dd62709cb77",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414246f2-cfff-4c30-9b1c-56ed5613c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### GRU ################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the GRU model with Dropout\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = GRUModel(max_events + 4, hidden_size, num_layers, output_size, dropout)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "print(\"Model Trained\")\n",
    "print(\"*\"*100 + \"\\n\")\n",
    "\n",
    "################################### Model Eval ################################\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_outputs = torch.sigmoid(test_outputs)  # Apply sigmoid to get probabilities\n",
    "    test_outputs = test_outputs.cpu().numpy()\n",
    "\n",
    "    # Convert y_test_tensor to NumPy array only if it is a tensor\n",
    "    if isinstance(y_test_tensor, torch.Tensor):\n",
    "        y_test_tensor = y_test_tensor.cpu().numpy()\n",
    "\n",
    "    test_preds = (test_outputs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "conf_matrix = confusion_matrix(y_test_tensor, test_preds)\n",
    "precision = precision_score(y_test_tensor, test_preds)\n",
    "recall = recall_score(y_test_tensor, test_preds)\n",
    "accuracy = accuracy_score(y_test_tensor, test_preds)\n",
    "f1 = f1_score(y_test_tensor, test_preds)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4aa4b-3906-481e-84f9-f0f451354614",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651de289-761c-4879-a272-596e4c0d6df1",
   "metadata": {},
   "source": [
    "## <font color = grey> Single - 1D-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338629df-6770-4b7e-a93e-12f84ad39670",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a7800-0ff9-4704-a705-08e5ed1b8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 1D CNN ################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the 1D CNN model with Dropout\n",
    "class CNN1DModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dropout=0.5):\n",
    "        super(CNN1DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        # Calculate the correct size after convolutions and pooling\n",
    "        self.fc1_input_size = 128 * (input_length // 2 // 2)\n",
    "        self.fc1 = nn.Linear(self.fc1_input_size, 128)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # Transpose to [batch_size, input_size, sequence_length]\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNN1DModel(max_events + 4, output_size, dropout)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "print(\"Model Trained\")\n",
    "print(\"*\"*100 + \"\\n\")\n",
    "\n",
    "################################### Model Eval ################################\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_outputs = torch.sigmoid(test_outputs)  # Apply sigmoid to get probabilities\n",
    "    test_outputs = test_outputs.cpu().numpy()\n",
    "\n",
    "    # Convert y_test_tensor to NumPy array only if it is a tensor\n",
    "    if isinstance(y_test_tensor, torch.Tensor):\n",
    "        y_test_tensor = y_test_tensor.cpu().numpy()\n",
    "\n",
    "    test_preds = (test_outputs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "conf_matrix = confusion_matrix(y_test_tensor, test_preds)\n",
    "precision = precision_score(y_test_tensor, test_preds)\n",
    "recall = recall_score(y_test_tensor, test_preds)\n",
    "accuracy = accuracy_score(y_test_tensor, test_preds)\n",
    "f1 = f1_score(y_test_tensor, test_preds)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042a1c4-7ec0-490b-82bb-256443d5e3a5",
   "metadata": {},
   "source": [
    "## <font color = grey> Single - Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e623ff-073e-46fb-a53a-9547f4460c92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b5109-534b-4be2-939b-a6d88a0a8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Transformer ################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.pos_encoder = nn.Embedding(input_length, hidden_size)\n",
    "        self.transformer = nn.Transformer(hidden_size, nhead=4, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        x = x + self.pos_encoder(positions)\n",
    "        x = self.transformer(x, x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(max_events + 4, hidden_size, num_layers, output_size, dropout)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "print(\"Model Trained\")\n",
    "print(\"*\"*100 + \"\\n\")\n",
    "\n",
    "################################### Model Eval ################################\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_outputs = torch.sigmoid(test_outputs)  # Apply sigmoid to get probabilities\n",
    "    test_outputs = test_outputs.cpu().numpy()\n",
    "\n",
    "    # Convert y_test_tensor to NumPy array only if it is a tensor\n",
    "    if isinstance(y_test_tensor, torch.Tensor):\n",
    "        y_test_tensor = y_test_tensor.cpu().numpy()\n",
    "\n",
    "    test_preds = (test_outputs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "conf_matrix = confusion_matrix(y_test_tensor, test_preds)\n",
    "precision = precision_score(y_test_tensor, test_preds)\n",
    "recall = recall_score(y_test_tensor, test_preds)\n",
    "accuracy = accuracy_score(y_test_tensor, test_preds)\n",
    "f1 = f1_score(y_test_tensor, test_preds)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8014b-bb97-434e-b46c-0639c9f51c85",
   "metadata": {},
   "source": [
    "# <font color = tomato> For Later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2092e4d-bd3b-4b8f-8601-4ef456471e29",
   "metadata": {},
   "source": [
    "## <font color = grey> Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d734f-6b62-4842-8e14-f893ed871637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################### Parameters and Hyperparameters ################################\n",
    "\n",
    "# # Parameters and Hyperparameters\n",
    "# data_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/10.20240705_StartOver_NewFeatures/01.Full_Base/'\n",
    "# input_file = f\"{data_dir}/20240704__full__new_features_v1.10.parquet\"\n",
    "# output_dir = '/home/ubuntu/efs-w210-capstone-ebs/11.Data/01.BGL/10.20240705_StartOver_NewFeatures/02.Full_Train_Test'\n",
    "\n",
    "# max_events_list = [10, 20]  # Different values to test\n",
    "# input_length_list = [10, 20]  # Different values to test\n",
    "# hidden_size_list = [64, 128]  # Different values to test\n",
    "# dropout_list = [0.3, 0.5]  # Different values to test\n",
    "\n",
    "# gap = 1\n",
    "# prediction_period = 1\n",
    "# test_size = 0.2\n",
    "# shuffle = False\n",
    "\n",
    "# num_layers = 2\n",
    "# output_size = 1\n",
    "# num_epochs = 50\n",
    "# batch_size = 16\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # DataFrame to track results\n",
    "# results_df = pd.DataFrame(columns=['Run', 'Precision', 'Recall', 'Accuracy', 'F1', 'TN', 'FP', 'FN', 'TP'])\n",
    "\n",
    "# run_id = 0\n",
    "\n",
    "# for max_events in max_events_list:\n",
    "#     for input_length in input_length_list:\n",
    "#         for hidden_size in hidden_size_list:\n",
    "#             for dropout in dropout_list:\n",
    "#                 run_id += 1\n",
    "#                 print(f\"Run {run_id}: max_events={max_events}, input_length={input_length}, hidden_size={hidden_size}, dropout={dropout}\")\n",
    "                \n",
    "#                 ################################### Data Preprocessing ################################\n",
    "\n",
    "#                 # Load the data\n",
    "#                 df = pd.read_parquet(input_file)\n",
    "\n",
    "#                 # Select derived features and a subset of EventID columns\n",
    "#                 selected_columns = ['time_start_int', 'Class', 'unique_events', 'most_frequent_event', 'transitions', 'entropy']\n",
    "#                 event_id_columns = [col for col in df.columns if col.startswith('EventId_')]\n",
    "\n",
    "#                 # Replace -1 values with 0\n",
    "#                 df[event_id_columns].replace(-1, 0, inplace=True)\n",
    "\n",
    "#                 # Scale numerical features\n",
    "#                 scaler = StandardScaler()\n",
    "#                 numerical_features = ['unique_events', 'transitions', 'entropy']\n",
    "#                 df[selected_columns[2:]] = scaler.fit_transform(df[selected_columns[2:]])\n",
    "\n",
    "#                 # Encode categorical features\n",
    "#                 label_encoder = LabelEncoder()\n",
    "#                 df['most_frequent_event'] = label_encoder.fit_transform(df['most_frequent_event'])\n",
    "\n",
    "#                 # Apply PCA to EventID columns to reduce them to 50 features\n",
    "#                 pca = PCA(n_components=max_events)\n",
    "#                 event_id_pca = pca.fit_transform(df[event_id_columns])\n",
    "\n",
    "#                 # Create a new DataFrame with the reduced EventID features\n",
    "#                 event_id_pca_df = pd.DataFrame(event_id_pca, columns=[f'EventId_PCA_{i+1}' for i in range(max_events)])\n",
    "\n",
    "#                 # Combine the reduced EventID features with the selected columns\n",
    "#                 df_reduced = pd.concat([df[selected_columns], event_id_pca_df], axis=1)\n",
    "\n",
    "#                 ################################### Create Sequences ################################\n",
    "\n",
    "#                 # Function to create non-overlapping sequences\n",
    "#                 def create_sequences(data, time_index_col, feature_cols, target_col, input_length, gap=1, prediction_period=1):\n",
    "#                     sequences = []\n",
    "#                     targets = []\n",
    "                    \n",
    "#                     start_idx = 0\n",
    "#                     while start_idx + input_length + gap + prediction_period <= len(data):\n",
    "#                         end_idx = start_idx + input_length\n",
    "#                         sequence = data[feature_cols].iloc[start_idx:end_idx].values\n",
    "#                         target = data[target_col].iloc[end_idx + gap:end_idx + gap + prediction_period].values[0]\n",
    "#                         sequences.append(sequence)\n",
    "#                         targets.append(target)\n",
    "#                         start_idx = end_idx + gap + prediction_period  # Move to the next non-overlapping sequence\n",
    "                    \n",
    "#                     return np.array(sequences), np.array(targets)\n",
    "\n",
    "#                 # feature_cols = selected_columns + selected_event_ids\n",
    "#                 feature_cols = [col for col in df_reduced.columns if col != 'Class']\n",
    "#                 target_col = 'Class'\n",
    "\n",
    "#                 X, y = create_sequences(df_reduced, 'time_start_int', feature_cols, target_col, input_length=input_length)\n",
    "\n",
    "#                 ################################### Split & SMOTE ################################\n",
    "\n",
    "#                 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "#                 # Flatten X_train to 2D array for SMOTE\n",
    "#                 X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "#                 # Apply SMOTE\n",
    "#                 smote = SMOTE(random_state=42)\n",
    "#                 X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#                 # Reshape X_train back to 3D array\n",
    "#                 X_train = X_train.reshape(-1, input_length, max_events + 5)\n",
    "\n",
    "#                 # Drop the first column from X_train and X_test\n",
    "#                 X_train = X_train[:, :, 1:]\n",
    "#                 X_test = X_test[:, :, 1:]\n",
    "\n",
    "#                 ################################### LSTM ################################\n",
    "\n",
    "#                 # Define the LSTM model with Dropout\n",
    "#                 class LSTMModel(nn.Module):\n",
    "#                     def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "#                         super(LSTMModel, self).__init__()\n",
    "#                         self.hidden_size = hidden_size\n",
    "#                         self.num_layers = num_layers\n",
    "#                         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "#                         self.fc = nn.Linear(hidden_size, output_size)\n",
    "                    \n",
    "#                     def forward(self, x):\n",
    "#                         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#                         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#                         out, _ = self.lstm(x, (h0, c0))\n",
    "#                         out = self.fc(out[:, -1, :])\n",
    "#                         return out\n",
    "\n",
    "#                 # Convert data to PyTorch tensors\n",
    "#                 X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "#                 y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "#                 X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "#                 y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#                 # Create DataLoader\n",
    "#                 train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#                 train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#                 # Initialize the model, loss function, and optimizer\n",
    "#                 model = LSTMModel(max_events + 4, hidden_size, num_layers, output_size, dropout)\n",
    "#                 criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "#                 optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#                 # Training loop\n",
    "#                 for epoch in range(num_epochs):\n",
    "#                     model.train()\n",
    "#                     for X_batch, y_batch in train_loader:\n",
    "#                         outputs = model(X_batch)\n",
    "#                         loss = criterion(outputs, y_batch)\n",
    "                        \n",
    "#                         optimizer.zero_grad()\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "                    \n",
    "#                     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "                    \n",
    "#                 ################################### Model Eval ################################\n",
    "\n",
    "#                 # Evaluate the model on the test set\n",
    "#                 model.eval()\n",
    "#                 with torch.no_grad():\n",
    "#                     test_outputs = model(X_test_tensor)\n",
    "#                     test_outputs = torch.sigmoid(test_outputs)  # Apply sigmoid to get probabilities\n",
    "#                     test_outputs = test_outputs.cpu().numpy()\n",
    "\n",
    "#                     # Convert y_test_tensor to NumPy array only if it is a tensor\n",
    "#                     if isinstance(y_test_tensor, torch.Tensor):\n",
    "#                         y_test_tensor = y_test_tensor.cpu().numpy()\n",
    "\n",
    "#                     test_preds = (test_outputs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "#                 # Calculate evaluation metrics\n",
    "#                 conf_matrix = confusion_matrix(y_test_tensor, test_preds)\n",
    "#                 precision = precision_score(y_test_tensor, test_preds)\n",
    "#                 recall = recall_score(y_test_tensor, test_preds)\n",
    "#                 accuracy = accuracy_score(y_test_tensor, test_preds)\n",
    "#                 f1 = f1_score(y_test_tensor, test_preds)\n",
    "#                 tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "#                 # Append results to DataFrame\n",
    "#                 results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "#                     'Run': run_id,\n",
    "#                     'Precision': precision,\n",
    "#                     'Recall': recall,\n",
    "#                     'Accuracy': accuracy,\n",
    "#                     'F1': f1,\n",
    "#                     'TN': tn,\n",
    "#                     'FP': fp,\n",
    "#                     'FN': fn,\n",
    "#                     'TP': tp\n",
    "#                 }])], ignore_index=True)\n",
    "\n",
    "#                 print(f\"Run {run_id} completed\")\n",
    "#                 print(\"*\"*100 + \"\\n\")\n",
    "\n",
    "# ################################### Plot Results ################################\n",
    "\n",
    "# # Plot the results\n",
    "# fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n",
    "# fig.suptitle('Model Performance Metrics Over Different Runs')\n",
    "\n",
    "# metrics = ['Precision', 'Recall', 'Accuracy', 'F1', 'TN', 'FP', 'FN', 'TP']\n",
    "# for i, metric in enumerate(metrics):\n",
    "#     ax = axes[i//2, i%2]\n",
    "#     ax.plot(results_df['Run'], results_df[metric])\n",
    "#     ax.set_title(metric)\n",
    "#     ax.set_xlabel('Run')\n",
    "#     ax.set_ylabel(metric)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5820f59-08d9-4b9c-abea-e5a96e6595f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.xlarge",
  "kernelspec": {
   "display_name": "20240705_TODS_StartOver",
   "language": "python",
   "name": "20240705_tods_startover"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
