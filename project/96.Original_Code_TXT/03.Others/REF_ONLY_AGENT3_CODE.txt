import requests
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score
from datetime import datetime
import re

# Parameters
data_dir = '/home/ubuntu/efs-w210-capstone-ebs/04A.Local_Data_Files'
save_dir = '/home/ubuntu/efs-w210-capstone-ebs/04C.Local_Inference_Eval_Files'
file_prefix = "06.20240714_062624_non_overlap_full_test"  # Replace with your actual file prefix

# Load the test data
X_test = np.load(f"{data_dir}/{file_prefix}_X_test.npy")
y_test = np.load(f"{data_dir}/{file_prefix}_y_test.npy")

print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

# Define start time and number of test runs
start_time = '2005-11-08 18:50:00'  # seq 11624
num_tests = 130

# Load the original parquet file with sequence mapping
input_file = f"{data_dir}/03.20240715_143154_orig_input_w_seq_info_FINAL.parquet"
original_df = pd.read_parquet(input_file)

# Ensure Seq_Num is integer for proper merging
original_df['Seq_Num'] = original_df['Seq_Num'].astype(int)

# Filter the test dataset based on the specified start time
start_seq_num = original_df[(original_df['Train_Test'] == 'Test') & (original_df['time_start'] == start_time)]['Seq_Num'].values[0]
print(f"Calculated start seq_num: {start_seq_num}")

# Define the number of records for each test run (20 source + 2 gap + 1 prediction period = 23 records)
num_records_per_test = 33

# Calculate the maximum end seq_num based on the dataset
max_end_seq_num = original_df[original_df['Train_Test'] == 'Test']['Seq_Num'].max()
print(f"Max end seq_num: {max_end_seq_num}")

# Calculate expected end seq_num based on user input
expected_end_seq_num = start_seq_num + num_tests * num_records_per_test - 1
print(f"Expected end seq_num based on user input: {expected_end_seq_num}")

# Check if the user-specified number of runs exceeds the maximum allowed runs
if expected_end_seq_num > max_end_seq_num:
    max_allowed_runs = (max_end_seq_num - start_seq_num + 1) // num_records_per_test
    print(f"Number of test runs exceeds the maximum allowed ({max_allowed_runs}). Setting num_tests to {max_allowed_runs}.")
    num_tests = max_allowed_runs
    expected_end_seq_num = start_seq_num + num_tests * num_records_per_test - 1

# Calculate the correct start and end sequence numbers for X_test and y_test
start_index_x_test = (start_seq_num - original_df[original_df['Train_Test'] == 'Test']['Seq_Num'].min()) // num_records_per_test
end_index_x_test = start_index_x_test + num_tests

# Ensure the end sequence number does not exceed the length of the test data
if end_index_x_test > len(X_test):
    end_index_x_test = len(X_test)

# Print the sequence numbers for debugging
print(f"Start seq_num for X_test: {start_index_x_test}")
print(f"End seq_num for X_test: {end_index_x_test}")

# Prepare the data for the specified number of test runs
X_test_limited = X_test[start_index_x_test:end_index_x_test]
y_test_limited = y_test[start_index_x_test:end_index_x_test]

# Print the shape and sample of the limited test data for debugging
print(f"X_test_limited shape: {X_test_limited.shape}")
print(f"X_test_limited sample: {X_test_limited[:1]}")

# Prepare the data for the request
data = {'X_test': X_test_limited.tolist()}

# Send a request to the server
response = requests.post('http://0.0.0.0:5000/predict', json=data)
response_data = response.json()

# Verify the response keys
print("Response keys:", response_data.keys())

# Process the response
predictions = np.array(response_data['predictions']).flatten()

# Print first 10 predictions and actual labels
print("First 10 predictions:", predictions[:10])
print("First 10 actual labels:", y_test_limited[:10])

# Create DataFrame for tracking and merging with additional information
tracking_data = []

for i in range(num_tests):
    total_seq_start = start_seq_num + i * num_records_per_test
    total_seq_end = total_seq_start + num_records_per_test - 1
    source_seq_start = total_seq_start
    source_seq_end = source_seq_start + 30 - 1
    gap_seq_start = source_seq_end + 1
    gap_seq_end = gap_seq_start + 1
    prediction_seq_start = total_seq_end - 1
    prediction_seq_end = total_seq_end
    
    tracking_data.append([
        total_seq_start, total_seq_end, source_seq_start, source_seq_end,
        gap_seq_start, gap_seq_end, prediction_seq_start, prediction_seq_end,
        predictions[i],  # Corrected to take the i-th prediction
        y_test_limited[i]  # Corresponding actual label
    ])

tracking_df = pd.DataFrame(tracking_data, columns=[
    "Total_Seq_Start", "Total_Seq_End", "Source_Seq_Start", "Source_Seq_End",
    "Gap_Seq_Start", "Gap_Seq_End", "Prediction_Seq_Start", "Prediction_Seq_End",
    "Predicted", "Actual"
])

# Convert Source_Seq_Start to integer for proper merging
tracking_df['Source_Seq_Start'] = tracking_df['Source_Seq_Start'].astype(int)

# Load the master tracking file
master_tracking_file = f"{data_dir}/03B.20240716_072206_orig_parquet_mapper_agents.parquet"
master_tracking_df = pd.read_parquet(master_tracking_file)

# Merge with original data to get necessary columns
merged_df = tracking_df.merge(master_tracking_df, left_on='Source_Seq_Start', right_on='Seq_Num', how='left')

# Retain only necessary columns
columns_to_keep = [
    "Total_Seq_Start", "Total_Seq_End", "Source_Seq_Start", "Source_Seq_End",
    "Gap_Seq_Start", "Gap_Seq_End", "Prediction_Seq_Start", "Prediction_Seq_End",
    "Predicted", "Actual", "Seq_Num", "Train_Test", "time_start", "time_start_int",
    "Class", "unique_events", "most_frequent_event", "transitions", "entropy",
    "hour_of_day", "day_of_week", "event_count", "top_event_frequency",
    "prev_event_count", "transition_rate", "high_transition_rate", "prev_entropy",
    "entropy_change", "rolling_event_count", "rolling_unique_event_count"
]

filtered_df = merged_df[columns_to_keep]

# Load event ID to template mapping file
eventid_encoding_file = f"{data_dir}/08.20240716031626_event_ID_int_template_mapping.csv"
eventid_encoding_df = pd.read_csv(eventid_encoding_file)

# Create a dictionary for quick lookup
eventid_to_template = dict(zip(eventid_encoding_df['EncodedValue'], eventid_encoding_df['EventTemplate']))

# Function to consolidate events and map to templates
def clean_text(text):
    # Remove unnecessary characters like <*>, <, and >
    text = re.sub(r'<\*?>', '', text)
    text = text.replace('<', '').replace('>', '')
    return text

def consolidate_events_to_text(df, start_col, end_col, col_name):
    consolidated = []
    for i, row in df.iterrows():
        seen_events = set()
        events = []
        for seq_num in range(row[start_col], row[end_col] + 1):
            event_list = master_tracking_df[master_tracking_df['Seq_Num'] == seq_num][col_name].astype(str).tolist()
            for event in event_list:
                for e in event.split(', '):
                    if e not in seen_events:
                        seen_events.add(e)
                        if int(e) in eventid_to_template:
                            cleaned_text = clean_text(eventid_to_template[int(e)])
                            events.append(f'"{cleaned_text}"')
                        else:
                            events.append('"Unknown"')
        consolidated.append(', '.join(events))
    return consolidated

# Add the new columns
filtered_df['runtime_most_frequent_consl_text'] = consolidate_events_to_text(filtered_df, 'Source_Seq_Start', 'Source_Seq_End', 'most_frequent_event')
filtered_df['runtime_least_frequent_consl_text'] = consolidate_events_to_text(filtered_df, 'Source_Seq_Start', 'Source_Seq_End', 'most_frequent_event')

# Display the head and tail of the final dataframe
display(filtered_df.head())
# display(filtered_df.tail())

# Define the timestamp
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')

# Save the consolidated DataFrame to the specified location
final_file = f"{save_dir}/03B.{timestamp}_agent3_non_overlap_model2_consl.csv"
filtered_df.to_csv(final_file, index=False)

print(f"Final file with consolidated columns saved to: {final_file}")

# Calculate metrics
conf_matrix = confusion_matrix(y_test_limited, predictions, labels=[0, 1])
precision = precision_score(y_test_limited, predictions, zero_division=0)
recall = recall_score(y_test_limited, predictions, zero_division=0)
accuracy = accuracy_score(y_test_limited, predictions)
f1 = f1_score(y_test_limited, predictions, zero_division=0)

# Check if the confusion matrix has the expected shape
if conf_matrix.size == 4:
    tn, fp, fn, tp = conf_matrix.ravel()
else:
    # Handle cases with fewer than two classes in the confusion matrix
    tn = conf_matrix[0, 0] if conf_matrix.shape[0] > 0 else 0
    fp = conf_matrix[0, 1] if conf_matrix.shape[1] > 1 else 0
    fn = conf_matrix[1, 0] if conf_matrix.shape[0] > 1 else 0
    tp = conf_matrix[1, 1] if conf_matrix.shape[1] > 1 else 0

# Create a DataFrame for metrics
metrics_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision (Class 1)', 'Recall (Class 1)', 'F1 Score', 'True Positives', 'False Positives', 'True Negatives', 'False Negatives'],
    'Value': [accuracy, precision, recall, f1, tp, fp, tn, fn]
})

# Print metrics DataFrame
print("\nMetrics:")
print(metrics_df)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()