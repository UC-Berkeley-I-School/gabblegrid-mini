# File: agents/historical_weather/agent_communication.py
import autogen  # Import autogen to fix the error
from autogen.cache import Cache
import os
import pandas as pd
import matplotlib.pyplot as plt

async def run_chat_weather_agents(user_proxy, manager, inference_params, experiment_id):
    task = f"""
    Your task is to run the get_historical_weather_data function with these parameters:
    max_events = {inference_params["max_events"]}
    input_length = {inference_params["input_length"]}
    gap = {inference_params["gap"]}
    prediction_period = {inference_params["prediction_period"]}
    selected_date = '{inference_params["selected_date"]}'
    selected_time = '{inference_params["selected_time"]}'
    num_tests = {inference_params["num_tests"]}
    experiment_id = '{experiment_id}'
    After running the function, please summarize the results in a concise email format.
    """

    try:
        with Cache.disk() as cache:
            await user_proxy.a_initiate_chat(
                manager, message=task, summary_method="reflection_with_llm", cache=cache
            )
    except Exception as e:
        st.error(f"Unexpected error: {e}")
        return None

    # Find the last message from log_historical_weather_writer
    writer_message = next((msg for msg in reversed(user_proxy.chat_messages[manager]) 
                           if isinstance(msg, dict) and msg.get('name') == 'log_historical_weather_writer'), None)

    if writer_message and 'content' in writer_message:
        results = writer_message['content']
        return results
    else:
        return "No summary generated by log_historical_weather_writer"

async def generate_plot(user_proxy, manager, plot_params):
    plot_task = f"""
    Your task is to run the plot_historical_weather_data function with this parameter:
    experiment_id = '{plot_params["experiment_id"]}',
    image_dir = '{plot_params["image_dir"]}'
    """
    try:
        with Cache.disk() as cache:
            await user_proxy.a_initiate_chat(manager, message=plot_task, summary_method="reflection_with_llm", cache=cache)
    except Exception as e:
        st.error(f"Unexpected error in generate_plot: {e}")
        return {"error": f"Unexpected error in generate_plot: {str(e)}"}

    # Find the last message from log_historical_weather_plotter
    plotter_message = next((msg for msg in reversed(user_proxy.chat_messages[manager]) 
                            if isinstance(msg, dict) and msg.get('name') == 'log_historical_weather_plotter'), None)

    if plotter_message and 'content' in plotter_message:
        plot_content = plotter_message['content']
        if isinstance(plot_content, dict) and 'plot_file_path' in plot_content:
            plot_path = plot_content['plot_file_path']
            if os.path.exists(plot_path):
                return {"plot_file_path": plot_path}
            else:
                return {"error": f"Plot file not found at {plot_path}"}
        else:
            return {"error": f"Invalid plot content: {plot_content}"}
    else:
        return {"error": "No response from log_historical_weather_plotter"}

def setup_weather_group_chat(agents, llm_config):
    groupchat = autogen.GroupChat(agents=list(agents.values()), messages=[], max_round=5)
    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)
    return groupchat, manager
